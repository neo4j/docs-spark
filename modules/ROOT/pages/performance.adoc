= Improving performance

To get the best possible performance reading from (and particularly writing to) Neo4j, make sure you've gone
through this checklist:

1. Tune your batch size.
2. Tune your Neo4j memory configuration.
3. Have the correct indexes.
4. Tune your parallelism.

Each of the following sections describes these in detail.

== Tune your batch size

Writing data to Neo4j happens transactionally in batches; if you want to write 1 million nodes, you might break
that into 40 batches of 25,000.  The batch size of the connector is controlled by the `batch.size` option and
is set to a fairly low, conservative level. _This is likely too low for many applications and can be improved
with better knowledge of your data_.

Batch size trade-off is as follows:

* The bigger the batch size, the better the overall ingest performance, because it means fewer transactions,
and less overall transactional overhead.
* When batch sizes become too large, so that Neo4j's heap memory cannot accommodate them, it can cause out of
memory errors on the server and cause failures.

[NOTE]
**Best write throughput comes when you use the largest batch size you can, while staying in the range of memory
available on the server.**

It's impossible to pick a single batch size that works for everyone, because how much memory your transactions
take up depends on the number of properties & relationships, and other factors.  A good general aggressive value
to try is around 20,000 - but you can increase this number if your data is small, or if you have a lot of memory
on the server.  Lower the number if it's a small database server, or the data your pushing has many large
properties.

== Tune your Neo4j memory configuration

In the link:https://neo4j.com/docs/operations-manual/current/performance/memory-configuration/[Neo4j Operations Manual], important
advice is given on how to size the heap and page cache of the server.  What's important for Spark is this:

* Heap affects how big transactions can get.  The bigger the heap, the larger the batch size you can use.
* Page cache affects how much of your database stays resident in RAM at any given time. Page caches which
are much smaller than your database cause performance to suffer.

== Have the correct indexes

At the Neo4j Cypher level, it's very common to use the Spark connector in a way that generates `MERGE` queries.
In Neo4j, this looks up a node by some 'key' and then creates it only if it does not already exist.

[NOTE]
**It is strongly recommended to assert indexes or constraints on any graph property that you use as part of
`node.keys`, `relationship.source.node.keys`, `relationship.target.node.keys` or other similar key options.**

A common source of poor performance is to write Spark code that generates `MERGE` Cypher, or otherwise tries
to look data up in Neo4j without the appropriate database indexes. In this case, the Neo4j server ends up looking
through much more data than necessary to satisfy the query, and performance suffers.

== Tune your parallelism

Spark is fundamentally about partitioning and parallelism; the go-to technique is to split a batch of
data into partitions for each machine to work on in parallel.   
In Neo4j, parallelism works very differently, which we will describe in this chapter.

=== Write parallelism in Neo4j

[NOTE]
**For most writes to Neo4j, it is strongly recommended to repartition your DataFrame to one partition only.**

When writing nodes and relationships in Neo4j:

* writing a relationship locks both nodes.
* writing a node locks the node.

Additionally, in the Neo4j Causal Cluster model, only the cluster leader may write data. Since writes scale vertically in Neo4j, the practical parallelism is limited to the number of cores on the leader.

The reason a single partition for writes is recommended is that it eliminates lock contention between writes. Suppose
one partition is writing:

```
(:Person { name: "Michael" })-[:KNOWS]->(:Person { name: "Andrea" })
```

While another partition is writing:

```
(:Person { name: "Andrea" })-[:KNOWS]->(:Person { name: "Davide" })
```

The relationship write locks the "Andrea" node - and these writes cannot continue in parallel in any case. As
a result, you may not gain performance by parallelizing more, if threads have to wait for each other's locks. In
extreme cases with too much parallelism, Neo4j may reject the writes with lock contention errors.

=== Dataset partitioning

[NOTE]
**You can use as many partitions as there are cores in the Neo4j server, if you have properly partitioned your data to avoid Neo4j locks.**

There is an exception to the "one partition" rule above: if your data writes are partitioned ahead of time to avoid locks, you
can generally do as many write threads to Neo4j as there are cores in the server. Suppose we want to write a long list of `:Person` nodes, and we know they are distinct by the person `id`. We might stream those into Neo4j in four different partitions, as there will not be any lock contention.

== Schema considerations

Neo4j does not have a fixed schema; individual properties can contain multiple differently typed values. Spark
on the other hand tends to expect a fixed schema. For this reason, the connector contains a number of schema
inference techniques that help ease this mapping. Paying close attention to how these features work can 
explain different scenarios.

The two core techniques are:

* <<APOC>>
* <<Automatic sampling>>

=== APOC

If your Neo4j installation has APOC installed, this approach is used by default. The stored procedures within APOC allow inspection of the
metadata in your graph and provide information such as the type of relationship properties and the universe of possible properties attached to a given node label.

You may try these calls by yourself on your Neo4j database if you wish, simply execute:

```cypher
CALL apoc.meta.nodeTypeProperties();
CALL apoc.meta.relTypeProperties();
```

Inspect the results.  These results are how the Neo4j Connector for Apache Spark represents the metadata of nodes and relationships read into DataFrames.

This approach uses a configurable sampling technique that looks through many (but not all) instances in the database to build a profile of the valid
values that exist within properties.  If the schema that is produced is not what is expected, take care to inspect the underlying data to ensure it has a consistent
property set across all nodes of a label, or investigate tuning the sampling approach.

==== Tune parameters

You can tune the configuration parameters of the https://neo4j.com/labs/apoc/4.1/database-introspection/meta/[two APOC procedures]
via the `option` method as it follows:

```scala
ss.read
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", "Product")
      .option("apoc.meta.nodeTypeProperties", """{"sample": 10}""")
      .load
```

or

```scala
ss.read
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "BOUGHT")
      .option("relationship.source.labels", "Product")
      .option("relationship.target.labels", "Person")
      .option("apoc.meta.relTypeProperties", """{"sample": 10}""")
      .load
```

For both procedures you can pass all the supported parameters except for:

* `includeLabels` for `apoc.meta.nodeTypeProperties`, because you use the labels defined in
the `labels` option.
* `includeRels` for `apoc.meta.relTypeProperties`, because you use the one defined in
the `relationship` option.

===== Fine tuning

As these two procedures sample the graph to extract the metadata necessary for building the <<Tables for labels>>,
in most real-world scenarios, it is crucial to tune the sampling parameters properly because using of them
can be expensive and impact the performance of your extraction job.

=== Automatic sampling

In some installations and environments, the key APOC calls above are not available.
In these cases, the connector automatically samples the first few records and infers
the correct data type from the examples that it sees.

[NOTE]
**Automatic sampling may be error prone and may produce incorrect results,
particularly in cases where a single Neo4j property exists with several different data types.
Consistent typing of properties is strongly recommended.**

== How can I speed up writes to Neo4j?

The Spark connector fundamentally writes data to Neo4j in batches. Neo4j is a transactional
database, and all modifications are made within a transaction. Those transactions in turn
have overhead.

The two simplest ways of increasing write performance are the following:

* You can increase the batch size (option `batch.size`). The larger the batch, the fewer transactions are executed to write all of your data, and the less transactional overhead is incurred.
* Ensure that your Neo4j instance has ample free heap and properly sized page cache. Small heaps make you unable to commit large batches, which in turn slows overall import.

[NOTE]
For best performance, make sure you are familiar with the material in the link:https://neo4j.com/docs/operations-manual/current/performance/[Operations manual].

It is important to keep in mind that Neo4j scales writes vertically and reads horizontally.  In
the link:https://neo4j.com/docs/operations-manual/current/clustering/introduction/[Causal Cluster Model], only the cluster leader (1 machine) may accept writes. For this reason, focus on getting the best hardware and performance on your cluster leader to maximize write throughput.