= Introduction

No matter which direction you're going, whether sending data from Spark to Neo4j or pulling data out of Neo4j,
the two systems are based on completely different organizing data models:

* Spark is oriented around tabular *DataFrames*.
* Neo4j is oriented around *graphs* (nodes, relationships, paths).

The use of this connector can be thought of as a "Transform" step and a "Load" step, regardless of
which direction the data is moving. 
The "Transform" step is concerned with how to move data between
graphs and tables; and the "Load" step is concerned with moving masses of data.

== Tables for labels

To move the data back and forth between Neo4j and Spark an approach called *Tables for Labels* is used. Consider the following path:

[source,cypher]
----
(:Customer)-[:BOUGHT]->(:Product)
----

You can decompose it into:

* two nodes: `Customer` and `Product`.
* one relationship between them: `BOUGHT`.

So in total, you have three graph entities, and each one is managed as a simple `table`.

So given the following Cypher query:

[source,cypher]
----
CREATE (c1:Customer{id:1, username: 'foo', city: 'Venezia'}),
(p1:Product{id: 100, name: 'My Awesome Product'}),
(c2:Customer{id:2, username: 'bar', city: 'Pescara'}),
(p2:Product{id: 101, name: 'My Awesome Product 2'}),
(c1)-[:BOUGHT{quantity: 10}]->(p1),
(c2)-[:BOUGHT{quantity: 13}]->(p1),
(c2)-[:BOUGHT{quantity: 4}]->(p2);
----

The graph structure is decomposed into three tables.

.Customer table
|===
|id |username |city

|1
|foo
|Venezia

|2
|bar
|Pescara
|===

.Product table
|===
|id |name

|100
|My Awesome Product

|101
|My Awesome Product 2
|===

.BOUGHT table
|===
|source_id |target_id | quantity

|1
|100
|10

|2
|100
|13

|2
|101
|4
|===

To have this decomposition you can use two strategies that allow to extract metadata from your graph:

* xref:performance.adoc#_apoc[APOC]
* xref:performance.adoc#_automatic_sampling[Automatic sampling]