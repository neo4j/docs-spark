= Schema optimization

Although Neo4j does not enforce the use of a schema, adding indexes and constraints before writing data makes the writing process more efficient.
When updating nodes or relationships, having constraints in place is also the best way to avoid duplicates.

// TODO remove
// The Spark Connector supports the following schema optimizations:
// 
// * <<indexes>> label:deprecated[]
// * Constraints on nodes
// ** <<node-constraints-unique>>
// ** <<node-constraints-key>>
// ** Property existence constraints
// ** Property type constraints
// * Constraints on relationships
// ** <<rel-constraints-unique>>
// ** <<rel-constraints-key>>
// ** Property existence constraints
// ** Property type constraints
// 
// * <<indexes>> label:deprecated[]
// * Property uniqueness constraints 
// ** On <<node-constraints-unique, nodes>>
// ** On <<rel-constraints-unique, relationships>>
// * Key constraints
// ** On <<node-constraints-key, nodes>>
// ** On <<rel-constraints-key, relationships>>
// * <<constraints-type>>
// * <<constraints-existence>>

The connector options for schema optimization are summarized below.

[CAUTION]
====
The schema optimization options described here cannot be used with the `query` option.
If you are using a xref:write/query.adoc[custom Cypher query], you need to create indexes and constraints manually using the xref:write/query.adoc#script-option[`script` option].
====

[NOTE]
====
TODO: confirm

Schema optimization only works with the `Overwrite` save mode.
====

[IMPORTANT]
====
label:new[New in 5.3]

The `schema.optimization.type` option is deprecated in favor of `schema.optimization.node.keys`, `schema.optimization.relationship.keys`, and `schema.optimization`.
====

.Schema optimization options
[cols="4, 2, 1, 3"]
|===
|Option|Value|Default|Description

|`schema.optimization.type` label:deprecated[Deprecated in 5.3]
|One of `NONE`, `INDEX`, `NODE_CONSTRAINTS`
|`NONE`
|Creates <<indexes, indexes>> and <<node-constraints-unique, property uniqueness>> constraints on nodes using the properties defined by the `node.keys` option.

|`schema.optimization.node.keys` label:new[New in 5.3]
|One of `UNIQUE`, `KEY`, `NONE`
|`NONE`
|Creates <<node-constraints-unique, property uniqueness>> and <<node-constraints-key, key>> constraints on nodes using the properties defined by the `node.keys` option.

|`schema.optimization.relationship.keys` label:new[New in 5.3]
|One of `UNIQUE`, `KEY`, `NONE`
|`NONE`
|Creates <<rel-constraints-unique, property uniqueness>> and <<rel-constraints-key, key>> constraints on relationships using the properties defined by the `relationship.keys` option.

|`schema.optimization` label:new[New in 5.3]
|Comma-separated list of `TYPE`, `EXISTS`, `NONE`
|`NONE`
|Creates <<constraints-existence, property existence>> and <<constraints-type, property type>> constraints on both nodes and relationships, enforcing the type and non-nullability from the DataFrame schema.
|===

[#indexes]
[role=label--deprecated]
== Indexes on node properties

link:https://neo4j.com/docs/cypher-manual/current/indexes/search-performance-indexes/overview/[Indexes] in Neo4j are often used to increase search performance.

You can create an index by setting the `schema.optimization.type` option to `INDEX`.
The index is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "surname")
  .option("schema.optimization.type", "INDEX")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE INDEX spark_INDEX_Person_surname FOR (n:Person) ON (n.surname)
----

The format of the index name is `spark_INDEX_<LABEL>_<NODE_KEYS>`, where `<LABEL>` is the first label from the `labels` option and `<NODE_KEYS>` is a dash-separated sequence of one or more properties as specified in the `node.keys` options.

[NOTE]
====
With multiple labels, only the first label is used to create the index.
====

[#node-constraints-unique]
== Node property uniqueness constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#unique-node-property[Node property uniqueness constraints] ensure that property values are unique for all nodes with a specific label.
For property uniqueness constraints on multiple properties, the combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.node.keys` option to `UNIQUE`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "name")
  .option("schema.optimization.node.keys", "UNIQUE")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_NODE_UNIQUE-CONSTRAINT_Person_name` IF NOT EXISTS FOR (e:Person) REQUIRE (e.name) IS UNIQUE
----

[NOTE]
====
With multiple labels, only the first label is used to create the index.
====

[NOTE]
====
You cannot create a uniqueness constraint on a node property if a key constraint already exists on the same property.
====

[IMPORTANT]
====
Before version *5.3.0*, node property uniqueness constraints could be added with the `schema.optimization.type` option set to `NODE_CONSTRAINTS`.
====

[#node-constraints-key]
== Node key constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#node-key[Node key constraints] ensure that, for a given node label and set of properties:

* All the properties exist on all the nodes with that label.
* The combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.node.keys` option to `KEY`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "name")
  .option("schema.optimization.node.keys", "KEY")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_NODE_KEY-CONSTRAINT_Person_name` IF NOT EXISTS FOR (e:Person) REQUIRE (e.name) IS NODE KEY
----

[NOTE]
====
You cannot create a key constraint on a node property if a uniqueness constraint already exists on the same property.
====

[#rel-constraints-unique]
== Relationship property uniqueness constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#unique-relationship-property[Relationship property uniqueness constraints] ensure that property values are unique for all relationships with a specific type.
For property uniqueness constraints on multiple properties, the combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.relationship.keys` option to `UNIQUE`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .mode(SaveMode.Overwrite)
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "PLAYS")
  .option("relationship.save.strategy", "keys")
  .option("relationship.source.labels", ":Musician")
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.node.keys", "name:name")
  .option("relationship.target.labels", ":Instrument")
  .option("relationship.target.node.keys", "instrument:name")
  .option("relationship.target.save.mode", "Overwrite")
  .option("schema.optimization.relationship.keys", "UNIQUE")
  .option("relationship.keys", "experience")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP_UNIQUE-CONSTRAINT_PLAYS_experience` IF NOT EXISTS FOR ()-[e:PLAYS]->() REQUIRE (e.experience) IS UNIQUE
----

[IMPORTANT]
====
The source and target nodes must already have a uniqueness constraint on the keys.
If not, the query will fail.
====

[NOTE]
====
You cannot create a uniqueness constraint on a relationship property if a key constraint already exists on the same property.
====

[#rel-constraints-key]
== Relationship key constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#relationship-key[Relationship key constraints] ensure that, for a given relationship type and set of properties:

* All the properties exist on all the relationships with that type.
* The combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.relationship.keys` option to `KEY`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .mode(SaveMode.Overwrite)
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "PLAYS")
  .option("relationship.save.strategy", "keys")
  .option("relationship.source.labels", ":Musician")
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.node.keys", "name:name")
  .option("relationship.target.labels", ":Instrument")
  .option("relationship.target.node.keys", "instrument:name")
  .option("relationship.target.save.mode", "Overwrite")
  .option("schema.optimization.relationship.keys", "KEY")
  .option("relationship.keys", "experience")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP_KEY-CONSTRAINT_PLAYS_experience` IF NOT EXISTS FOR ()-[e:PLAYS]->() REQUIRE (e.experience) IS RELATIONSHIP KEY
----

[IMPORTANT]
====
The source and target nodes must already have a uniqueness constraint on the keys.
If not, the query will fail.
====

[NOTE]
====
You cannot create a key constraint on a relationship property if a uniqueness constraint already exists on the same property.
====

[#constraints-type]
== Property type constraints

Since Neo4j 5.11 the database allows to create type constraints for node and relationship properties.
In order to leverage this feature we added the option `schema.optimization` that will use the DataFrame schema in order to enforce the type.
Internally the connector will use the following mapping:

.Spark to Cypher constraint type mapping
|===
|Spark type |Neo4j Type
|BooleanType |BOOLEAN
|StringType |STRING
|IntegerType |INTEGER
|LongType |INTEGER
|FloatType |FLOAT
|DoubleType |FLOAT
|DateType |DATE
|TimestampType |LOCAL DATETIME
|Custom `pointType` as: Struct { type: string, srid: integer, x: double, y: double, z: double }| POINT
|Custom `durationType` as: Struct { type: string, months: long, days: long, seconds: long, nanonseconds: integer, value: string }| DURATION
|DataTypes.createArrayType(BooleanType, false) |LIST<BOOLEAN NOT NULL>
|DataTypes.createArrayType(StringType, false) |LIST<STRING NOT NULL>
|DataTypes.createArrayType(IntegerType, false) |LIST<INTEGER NOT NULL>
|DataTypes.createArrayType(LongType, false) |LIST<INTEGER NOT NULL>
|DataTypes.createArrayType(FloatType, false) |LIST<FLOAT NOT NULL>
|DataTypes.createArrayType(DoubleType, false) |LIST<FLOAT NOT NULL>
|DataTypes.createArrayType(DateType, false) |LIST<DATE NOT NULL>
|DataTypes.createArrayType(TimestampType, false) |LIST<LOCAL DATETIME NOT NULL>
|DataTypes.createArrayType(pointType, false) |LIST<POINT NOT NULL>
|DataTypes.createArrayType(durationType, false) |LIST<DURATION NOT NULL>

|===

For the arrays in particular we use the version without null elements as Neo4j does not allow to have them in arrays.

You can leverage this kind of schema enforcement with the value `TYPE`.

[#constraints-existence]
== Property existence constraints

Neo4j defines "property existence" as a synonym for NOT NULL condition.
You can leverage this kind of schema enforcement with the value `EXISTS`, the connector will use the nullability of the DataFrame column to choose whether to apply or not the NOT NULL condition.

=== Node Property type and existence constraints

Given the following example:

[source, scala]
----
    ds.write
      .format("org.neo4j.spark.DataSource")
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization", "TYPE,EXISTS")
      .save()
----

The connector will create, for each dataframe column a type constraint for the label `Person` according with the mapping table provided above.

The constraint query looks like the following:

[source, cypher]
----
CREATE CONSTRAINT `spark_NODE-TYPE-CONSTRAINT-Person-surname` IF NOT EXISTS FOR (e:Person) REQUIRE e.surname IS :: STRING
----

If the DataFrame schema says that the field is also NOT NULL the connector creates an existence constraint as it follows:

[source, cypher]
----
CREATE CONSTRAINT `spark_NODE-NOT_NULL-CONSTRAINT-Person-surname` IF NOT EXISTS FOR (e:Person) REQUIRE e.surname IS NOT NULL
----

=== Relationship Property type and existence constraints

Given the following example:

[source, scala]
----
    ds.write
      .mode(SaveMode.Overwrite)
      .format("org.neo4j.spark.DataSource")
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "MY_REL")
      .option("relationship.save.strategy", "keys")
      .option("relationship.source.labels", ":NodeA")
      .option("relationship.source.save.mode", "Overwrite")
      .option("relationship.source.node.keys", "idSource:id")
      .option("relationship.target.labels", ":NodeB")
      .option("relationship.target.node.keys", "idTarget:id")
      .option("relationship.target.save.mode", "Overwrite")
      .option("schema.optimization", "TYPE,EXISTS")
      .save()
----

The connector will create:

* a type constraint for node `NodeA` and property `id`
* a type constraint for node `NodeB` and property `id`
* all the remaining properties are used as relationship properties; for each property a type constraint is created for the relationship `MY_REL` by using the following query:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP-TYPE-CONSTRAINT-MY_REL-foo` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE e.foo IS :: STRING
----

If the DataFrame schema says that the field is also NOT NULL the connector creates an existence constraint as it follows:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP-NOT_NULL-CONSTRAINT-MY_REL-foo` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE e.foo IS NOT NULL
----

The constraint is not recreated if it is already present.