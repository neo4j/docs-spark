= Schema optimization

Although Neo4j does not enforce the use of a schema, adding indexes and constraints before writing data makes the writing process more efficient.
When updating nodes or relationships, having constraints in place is also the best way to avoid duplicates.

// TODO remove
// The Spark Connector supports the following schema optimizations:
// 
// * <<indexes>> label:deprecated[]
// * Constraints on nodes
// ** <<node-constraints-unique>>
// ** <<node-constraints-key>>
// ** Property existence constraints
// ** Property type constraints
// * Constraints on relationships
// ** <<rel-constraints-unique>>
// ** <<rel-constraints-key>>
// ** Property existence constraints
// ** Property type constraints
// 
// * <<indexes>> label:deprecated[]
// * Property uniqueness constraints 
// ** On <<node-constraints-unique, nodes>>
// ** On <<rel-constraints-unique, relationships>>
// * Key constraints
// ** On <<node-constraints-key, nodes>>
// ** On <<rel-constraints-key, relationships>>
// * <<constraints-type>>
// * <<constraints-existence>>

The connector options for schema optimization are summarized below.

[CAUTION]
====
The schema optimization options described here cannot be used with the `query` option.
If you are using a xref:write/query.adoc[custom Cypher query], you need to create indexes and constraints manually using the xref:write/query.adoc#script-option[`script` option].
====

[IMPORTANT]
====
label:new[New in 5.3]

The `schema.optimization.type` option is deprecated in favor of `schema.optimization.node.keys`, `schema.optimization.relationship.keys`, and `schema.optimization`.
====

.Schema optimization options
[cols="4, 2, 1, 3"]
|===
|Option|Value|Default|Description

|`schema.optimization.type` label:deprecated[Deprecated in 5.3]
|One of `NONE`, `INDEX`, `NODE_CONSTRAINTS`
|`NONE`
|Creates <<indexes, indexes>> and <<node-constraints-unique, property uniqueness>> constraints on nodes using the properties defined by the `node.keys` option.

|`schema.optimization.node.keys` label:new[New in 5.3]
|One of `UNIQUE`, `KEY`, `NONE`
|`NONE`
|Creates <<node-constraints-unique, property uniqueness>> and <<node-constraints-key, key>> constraints on nodes using the properties defined by the `node.keys` option.

|`schema.optimization.relationship.keys` label:new[New in 5.3]
|One of `UNIQUE`, `KEY`, `NONE`
|`NONE`
|Creates <<rel-constraints-unique, property uniqueness>> and <<rel-constraints-key, key>> constraints on relationships using the properties defined by the `relationship.keys` option.

|`schema.optimization` label:new[New in 5.3]
|Comma-separated list of `TYPE`, `EXISTS`, `NONE`
|`NONE`
|Creates <<constraints-type-existence, property type and property existence>> constraints on both nodes and relationships, enforcing the type and non-nullability from the DataFrame schema.
|===

[#indexes]
[role=label--deprecated]
== Indexes on node properties

link:https://neo4j.com/docs/cypher-manual/current/indexes/search-performance-indexes/overview/[Indexes] in Neo4j are often used to increase search performance.

You can create an index by setting the `schema.optimization.type` option to `INDEX`.
The index is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "surname")
  .option("schema.optimization.type", "INDEX")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE INDEX spark_INDEX_Person_surname FOR (n:Person) ON (n.surname)
----

The format of the index name is `spark_INDEX_<LABEL>_<NODE_KEYS>`, where `<LABEL>` is the first label from the `labels` option and `<NODE_KEYS>` is a dash-separated sequence of one or more properties as specified in the `node.keys` options.

[NOTE]
====
With multiple labels, only the first label is used to create the index.
====

[#node-constraints-unique]
== Node property uniqueness constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#unique-node-property[Node property uniqueness constraints] ensure that property values are unique for all nodes with a specific label.
For property uniqueness constraints on multiple properties, the combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.node.keys` option to `UNIQUE`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "name")
  .option("schema.optimization.node.keys", "UNIQUE")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_NODE_UNIQUE-CONSTRAINT_Person_name` IF NOT EXISTS FOR (e:Person) REQUIRE (e.name) IS UNIQUE
----

[NOTE]
====
With multiple labels, only the first label is used to create the index.
====

[NOTE]
====
You cannot create a uniqueness constraint on a node property if a key constraint already exists on the same property.
====

[NOTE]
====
This schema optimization only works with the `Overwrite` save mode.
====

[IMPORTANT]
====
Before version *5.3.0*, node property uniqueness constraints could be added with the `schema.optimization.type` option set to `NODE_CONSTRAINTS`.
====

[#node-constraints-key]
== Node key constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#node-key[Node key constraints] ensure that, for a given node label and set of properties:

* All the properties exist on all the nodes with that label.
* The combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.node.keys` option to `KEY`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "name")
  .option("schema.optimization.node.keys", "KEY")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_NODE_KEY-CONSTRAINT_Person_name` IF NOT EXISTS FOR (e:Person) REQUIRE (e.name) IS NODE KEY
----

[NOTE]
====
You cannot create a key constraint on a node property if a uniqueness constraint already exists on the same property.
====

[NOTE]
====
This schema optimization only works with the `Overwrite` save mode.
====

[#rel-constraints-unique]
== Relationship property uniqueness constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#unique-relationship-property[Relationship property uniqueness constraints] ensure that property values are unique for all relationships with a specific type.
For property uniqueness constraints on multiple properties, the combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.relationship.keys` option to `UNIQUE`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .mode(SaveMode.Overwrite)
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "PLAYS")
  .option("relationship.save.strategy", "keys")
  .option("relationship.source.labels", ":Musician")
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.node.keys", "name:name")
  .option("relationship.target.labels", ":Instrument")
  .option("relationship.target.node.keys", "instrument:name")
  .option("relationship.target.save.mode", "Overwrite")
  .option("schema.optimization.relationship.keys", "UNIQUE")
  .option("relationship.keys", "experience")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP_UNIQUE-CONSTRAINT_PLAYS_experience` IF NOT EXISTS FOR ()-[e:PLAYS]->() REQUIRE (e.experience) IS UNIQUE
----

[IMPORTANT]
====
The source and target nodes must already have a uniqueness constraint on the keys.
If not, the query will fail.
====

[NOTE]
====
You cannot create a uniqueness constraint on a relationship property if a key constraint already exists on the same property.
====

[NOTE]
====
This schema optimization only works with the `Overwrite` save mode.
====

[#rel-constraints-key]
== Relationship key constraints

link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#relationship-key[Relationship key constraints] ensure that, for a given relationship type and set of properties:

* All the properties exist on all the relationships with that type.
* The combination of the property values is unique.

You can create a constraint by setting the `schema.optimization.relationship.keys` option to `KEY`.
The constraint is not recreated if it is already present.

.Example
[source, scala]
----
df.write
  .mode(SaveMode.Overwrite)
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "PLAYS")
  .option("relationship.save.strategy", "keys")
  .option("relationship.source.labels", ":Musician")
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.node.keys", "name:name")
  .option("relationship.target.labels", ":Instrument")
  .option("relationship.target.node.keys", "instrument:name")
  .option("relationship.target.save.mode", "Overwrite")
  .option("schema.optimization.relationship.keys", "KEY")
  .option("relationship.keys", "experience")
  .save()
----

Before the writing process starts, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP_KEY-CONSTRAINT_PLAYS_experience` IF NOT EXISTS FOR ()-[e:PLAYS]->() REQUIRE (e.experience) IS RELATIONSHIP KEY
----

[IMPORTANT]
====
The source and target nodes must already have a uniqueness constraint on the keys.
If not, the query will fail.
====

[NOTE]
====
You cannot create a key constraint on a relationship property if a uniqueness constraint already exists on the same property.
====

[NOTE]
====
This schema optimization only works with the `Overwrite` save mode.
====

[#constraints-type-existence]
== Property type and property existence constraints

Property type constraints ensure that a property have the required property type for all nodes with a specific label (link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#node-property-type[node property type constraints]) or for all relationships with a specific type (link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#relationship-property-type[relationship property type constraints]).

The connector uses the DataFrame schema in order to enforce the type.

Property existence constraints ensure that a property exists (`IS NOT NULL`) for all nodes with a specific label (link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#node-property-existence[node property type constraints]) or for all relationships with a specific type (link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/#relationship-property-existence[relationship property type constraints]).

The connector uses the nullability of the DataFrame column to choose whether to apply or not the `NOT NULL` condition.

You can create property type constraints for both nodes and relationships by setting the `schema.optimization` option to `TYPE`.
You can create property existence constraints for both nodes and relationships by setting the `schema.optimization` option to `EXISTS`.
You can create both at the same time by setting the `schema.optimization` option to `TYPE,EXISTS`.
The constraints are not recreated if it is already present.

Internally the connector will use the following mapping:

.Spark to Cypher constraint type mapping
|===
|Spark type |Neo4j Type
|BooleanType |BOOLEAN
|StringType |STRING
|IntegerType |INTEGER
|LongType |INTEGER
|FloatType |FLOAT
|DoubleType |FLOAT
|DateType |DATE
|TimestampType |LOCAL DATETIME
|Custom `pointType` as: Struct { type: string, srid: integer, x: double, y: double, z: double }| POINT
|Custom `durationType` as: Struct { type: string, months: long, days: long, seconds: long, nanonseconds: integer, value: string }| DURATION
|DataTypes.createArrayType(BooleanType, false) |LIST<BOOLEAN NOT NULL>
|DataTypes.createArrayType(StringType, false) |LIST<STRING NOT NULL>
|DataTypes.createArrayType(IntegerType, false) |LIST<INTEGER NOT NULL>
|DataTypes.createArrayType(LongType, false) |LIST<INTEGER NOT NULL>
|DataTypes.createArrayType(FloatType, false) |LIST<FLOAT NOT NULL>
|DataTypes.createArrayType(DoubleType, false) |LIST<FLOAT NOT NULL>
|DataTypes.createArrayType(DateType, false) |LIST<DATE NOT NULL>
|DataTypes.createArrayType(TimestampType, false) |LIST<LOCAL DATETIME NOT NULL>
|DataTypes.createArrayType(pointType, false) |LIST<POINT NOT NULL>
|DataTypes.createArrayType(durationType, false) |LIST<DURATION NOT NULL>

|===

For the arrays in particular we use the version without null elements as Neo4j does not allow to have them in arrays.

=== On nodes

.Example
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "surname")
  .option("schema.optimization", "TYPE,EXISTS")
  .save()
----

Before the writing process starts, the connector runs the following schema queries (one query for each DataFrame column):

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_NODE-TYPE-CONSTRAINT-Person-name` IF NOT EXISTS FOR (e:Person) REQUIRE e.name IS :: STRING

CREATE CONSTRAINT `spark_NODE-TYPE-CONSTRAINT-Person-surname` IF NOT EXISTS FOR (e:Person) REQUIRE e.surname IS :: STRING

CREATE CONSTRAINT `spark_NODE-TYPE-CONSTRAINT-Person-age` IF NOT EXISTS FOR (e:Person) REQUIRE e.age IS :: INTEGER
----

If a DataFrame column is not nullable, the connector runs additional schema queries.
For example, if the `age` column is not nullable, the connector runs the following schema query:

.Schema query
[source, cypher]
----
CREATE CONSTRAINT `spark_NODE-NOT_NULL-CONSTRAINT-Person-age` IF NOT EXISTS FOR (e:Person) REQUIRE e.age IS NOT NULL
----

=== On relationships TODO

Given the following example:

[source, scala]
----
    ds.write
      .mode(SaveMode.Overwrite)
      .format("org.neo4j.spark.DataSource")
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "MY_REL")
      .option("relationship.save.strategy", "keys")
      .option("relationship.source.labels", ":NodeA")
      .option("relationship.source.save.mode", "Overwrite")
      .option("relationship.source.node.keys", "idSource:id")
      .option("relationship.target.labels", ":NodeB")
      .option("relationship.target.node.keys", "idTarget:id")
      .option("relationship.target.save.mode", "Overwrite")
      .option("schema.optimization", "TYPE,EXISTS")
      .save()
----

The connector will create:

* a type constraint for node `NodeA` and property `id`
* a type constraint for node `NodeB` and property `id`
* all the remaining properties are used as relationship properties; for each property a type constraint is created for the relationship `MY_REL` by using the following query:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP-TYPE-CONSTRAINT-MY_REL-foo` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE e.foo IS :: STRING
----

If the DataFrame schema says that the field is also NOT NULL the connector creates an existence constraint as it follows:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP-NOT_NULL-CONSTRAINT-MY_REL-foo` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE e.foo IS NOT NULL
----

The constraint is not recreated if it is already present.
