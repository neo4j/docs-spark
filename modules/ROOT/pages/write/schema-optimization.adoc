= Schema optimization

Although Neo4j does not enforce the use of a schema, adding indexes and constraints before writing data makes the writing process more efficient.
When updating nodes or relationships, having constraints in place is also the best way to avoid duplicates.

// TODO remove
// The Spark Connector supports the following schema optimizations:
// 
// * <<indexes>> label:deprecated[]
// * Constraints on nodes
// ** <<node-constraints-unique>>
// ** <<node-constraints-key>>
// ** Property existence constraints
// ** Property type constraints
// * Constraints on relationships
// ** <<rel-constraints-unique>>
// ** <<rel-constraints-key>>
// ** Property existence constraints
// ** Property type constraints
// 
// * <<indexes>> label:deprecated[]
// * Property uniqueness constraints 
// ** On <<node-constraints-unique, nodes>>
// ** On <<rel-constraints-unique, relationships>>
// * Key constraints
// ** On <<node-constraints-key, nodes>>
// ** On <<rel-constraints-key, relationships>>
// * <<constraints-type>>
// * <<constraints-existence>>

The connector provides the following options for schema optimization.

[CAUTION]
====
The schema optimization options described here cannot be used with the `query` option.
If you are using a xref:write/query.adoc[custom Cypher query], you need to create indexes and constraints manually using the xref:write/query.adoc#script-option[`script` option].
====

[NOTE]
====
label:new[New in 5.3]

The `schema.optimization.type` option is deprecated in favor of `schema.optimization.node.keys`, `schema.optimization.relationship.keys`, and `schema.optimization`.
====

.Schema optimization options
[cols="4, 2, 1, 3"]
|===
|Option|Value|Default|Description

|`schema.optimization.type` label:deprecated[Deprecated in 5.3]
|One of `NONE`, `INDEX`, `NODE_CONSTRAINTS`
|`NONE`
|Creates <<indexes, indexes>> and <<node-constraints-unique, property uniqueness>> constraints on nodes.

|`schema.optimization.node.keys` label:new[New in 5.3]
|One of `UNIQUE`, `KEY`, `NONE`
|`NONE`
|Creates <<node-constraints-unique, property uniqueness>> and <<node-constraints-key, key>> constraints on nodes using the properties defined by the `node.keys` option.

|`schema.optimization.relationship.keys` label:new[New in 5.3]
|One of `UNIQUE`, `KEY`, `NONE`
|`NONE`
|Creates <<rel-constraints-unique, property uniqueness>> and <<rel-constraints-key, key>> constraints on relationships using the properties defined by the `relationship.keys` option.

|`schema.optimization` label:new[New in 5.3]
|Comma-separated list of `TYPE`, `EXISTS`, `NONE`
|`NONE`
|Creates <<constraints-existence, property existence>> and <<constraints-type, property type>> constraints on both nodes and relationships, enforcing the type and non-nullability from the DataFrame schema.
|===

[#indexes]
[role=label--deprecated]
== Indexes on nodes

The following example shows how to create indexes while you're creating nodes.

----
ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.type", "INDEX")
      .save()
----

Before the import starts, the following schema query is being created:

----
CREATE INDEX ON :Person(surname)
----

The name of the created index is `spark_INDEX_<LABEL>_<NODE_KEYS>`, where `<LABEL>` is the first label from the `labels` option and `<NODE_KEYS>` is a dash-separated sequence of one or more properties as specified in the `node.keys` options.
In this example, the name of the created index is `spark_INDEX_Person_surname`.
If the `node.keys` option were set to `"name,surname"` instead, the index name would be `spark_INDEX_Person_name-surname`.

The index is not recreated if it is already present.

[#node-constraints-unique]
== Node property uniqueness constraints

*Take into consideration that the first label is used for the index creation.*

[NOTE]
For a detailed description of how Neo4j handles unique constraints on nodes see the https://neo4j.com/docs/cypher-manual/current/constraints/#unique-node-property[Cypher documentation]

Given the following example:

[source, scala]
----
    ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.node.keys", "UNIQUE")
      .save()
----

Under the hood the Spark connector will create the following constraint:

[source, cypher]
----
CREATE CONSTRAINT `spark_NODE_UNIQUE-CONSTRAINT_Person_surname` IF NOT EXISTS FOR (e:Person) REQUIRE (e.surname) IS UNIQUE
----

[NOTE]
====
Before version *5.3.0*, node property uniqueness constraints could be added with the `schema.optimization.type` option set to `NODE_CONSTRAINTS`.
====

[#node-constraints-key]
== Node key constraints

[NOTE]
For a detailed description of how Neo4j handles node key constraints see the https://neo4j.com/docs/cypher-manual/current/constraints/#node-key[Cypher documentation]

Given the following example:

[source, scala]
----
    ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.node.keys", "KEY")
      .save()
----

Under the hood the Spark connector will create the following constraint:

[source, cypher]
----
CREATE CONSTRAINT `spark_NODE_KEY-CONSTRAINT_Person_surname` IF NOT EXISTS FOR (e:Person) REQUIRE (e.surname) IS NODE KEY
----

[#rel-constraints-unique]
== Relationship property uniqueness constraints

[NOTE]
For a detailed description of how Neo4j handles unique constraints on relationships see the official https://neo4j.com/docs/cypher-manual/current/constraints/#unique-relationship-property[Cypher documentation]

Given the following example:

[source, scala]
----
    ds
      .write
      .mode(SaveMode.Overwrite)
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "MY_REL")
      .option("relationship.save.strategy", "keys")
      .option("relationship.source.labels", ":NodeA")
      .option("relationship.source.save.mode", "Overwrite")
      .option("relationship.source.node.keys", "idSource:id")
      .option("relationship.target.labels", ":NodeB")
      .option("relationship.target.node.keys", "idTarget:id")
      .option("relationship.target.save.mode", "Overwrite")
      .option("schema.optimization.relationship.keys", "UNIQUE")
      .option("relationship.keys", "foo,bar")
      .save()
----

Under the hood the Spark connector will create the following constraint:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP_UNIQUE-CONSTRAINT_MY_REL_foo-bar` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE (e.foo, e.bar) IS UNIQUE
----

[#rel-constraints-key]
== Relationship key constraints

[NOTE]
For a detailed description of how Neo4j handles relationship key constraint see the official https://neo4j.com/docs/cypher-manual/current/constraints/#relationship-key[Cypher documentation]

Given the following example:

[source, scala]
----
    ds
      .write
      .mode(SaveMode.Overwrite)
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "MY_REL")
      .option("relationship.save.strategy", "keys")
      .option("relationship.source.labels", ":NodeA")
      .option("relationship.source.save.mode", "Overwrite")
      .option("relationship.source.node.keys", "idSource:id")
      .option("relationship.target.labels", ":NodeB")
      .option("relationship.target.node.keys", "idTarget:id")
      .option("relationship.target.save.mode", "Overwrite")
      .option("schema.optimization.relationship.keys", "KEY")
      .option("relationship.keys", "foo,bar")
      .save()
----

Under the hood the Spark connector will create the following constraint:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP_KEY-CONSTRAINT_MY_REL_foo-bar` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE (e.foo, e.bar) IS RELATIONSHIP KEY
----

[#constraints-type]
== Property type constraints

Since Neo4j 5.11 the database allows to create type constraints for node and relationship properties.
In order to leverage this feature we added the option `schema.optimization` that will use the DataFrame schema in order to enforce the type.
Internally the connector will use the following mapping:

.Spark to Cypher constraint type mapping
|===
|Spark type |Neo4j Type
|BooleanType |BOOLEAN
|StringType |STRING
|IntegerType |INTEGER
|LongType |INTEGER
|FloatType |FLOAT
|DoubleType |FLOAT
|DateType |DATE
|TimestampType |LOCAL DATETIME
|Custom `pointType` as: Struct { type: string, srid: integer, x: double, y: double, z: double }| POINT
|Custom `durationType` as: Struct { type: string, months: long, days: long, seconds: long, nanonseconds: integer, value: string }| DURATION
|DataTypes.createArrayType(BooleanType, false) |LIST<BOOLEAN NOT NULL>
|DataTypes.createArrayType(StringType, false) |LIST<STRING NOT NULL>
|DataTypes.createArrayType(IntegerType, false) |LIST<INTEGER NOT NULL>
|DataTypes.createArrayType(LongType, false) |LIST<INTEGER NOT NULL>
|DataTypes.createArrayType(FloatType, false) |LIST<FLOAT NOT NULL>
|DataTypes.createArrayType(DoubleType, false) |LIST<FLOAT NOT NULL>
|DataTypes.createArrayType(DateType, false) |LIST<DATE NOT NULL>
|DataTypes.createArrayType(TimestampType, false) |LIST<LOCAL DATETIME NOT NULL>
|DataTypes.createArrayType(pointType, false) |LIST<POINT NOT NULL>
|DataTypes.createArrayType(durationType, false) |LIST<DURATION NOT NULL>

|===

For the arrays in particular we use the version without null elements as Neo4j does not allow to have them in arrays.

You can leverage this kind of schema enforcement with the value `TYPE`.

[#constraints-existence]
== Property existence constraints

Neo4j defines "property existence" as a synonym for NOT NULL condition.
You can leverage this kind of schema enforcement with the value `EXISTS`, the connector will use the nullability of the DataFrame column to choose whether to apply or not the NOT NULL condition.

=== Node Property type and existence constraints

Given the following example:

[source, scala]
----
    ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization", "TYPE,EXISTS")
      .save()
----

The connector will create, for each dataframe column a type constraint for the label `Person` according with the mapping table provided above.

The constraint query looks like the following:

[source, cypher]
----
CREATE CONSTRAINT `spark_NODE-TYPE-CONSTRAINT-Person-surname` IF NOT EXISTS FOR (e:Person) REQUIRE e.surname IS :: STRING
----

If the DataFrame schema says that the field is also NOT NULL the connector creates an existence constraint as it follows:

[source, cypher]
----
CREATE CONSTRAINT `spark_NODE-NOT_NULL-CONSTRAINT-Person-surname` IF NOT EXISTS FOR (e:Person) REQUIRE e.surname IS NOT NULL
----

=== Relationship Property type and existence constraints

Given the following example:

[source, scala]
----
    ds.write
      .mode(SaveMode.Overwrite)
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "MY_REL")
      .option("relationship.save.strategy", "keys")
      .option("relationship.source.labels", ":NodeA")
      .option("relationship.source.save.mode", "Overwrite")
      .option("relationship.source.node.keys", "idSource:id")
      .option("relationship.target.labels", ":NodeB")
      .option("relationship.target.node.keys", "idTarget:id")
      .option("relationship.target.save.mode", "Overwrite")
      .option("schema.optimization", "TYPE,EXISTS")
      .save()
----

The connector will create:

* a type constraint for node `NodeA` and property `id`
* a type constraint for node `NodeB` and property `id`
* all the remaining properties are used as relationship properties; for each property a type constraint is created for the relationship `MY_REL` by using the following query:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP-TYPE-CONSTRAINT-MY_REL-foo` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE e.foo IS :: STRING
----

If the DataFrame schema says that the field is also NOT NULL the connector creates an existence constraint as it follows:

[source, cypher]
----
CREATE CONSTRAINT `spark_RELATIONSHIP-NOT_NULL-CONSTRAINT-MY_REL-foo` IF NOT EXISTS FOR ()-[e:MY_REL]->() REQUIRE e.foo IS NOT NULL
----

The constraint is not recreated if it is already present.