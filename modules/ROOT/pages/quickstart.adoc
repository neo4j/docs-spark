= Quickstart
:description: This chapter describes the quick way to get started with Neo4j Connector for Apache Spark.

You can create Spark DataFrames and write them to your Neo4j database:

.Write the DataFrame to nodes of label `Person`
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val df = List(
  ("John Doe", 32),
  ("Jane Doe", 42),
).toDF("name", "age")

(df.write.format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Append)
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("labels", ":Person")
  .save())
----

Similarly, you can read from your Neo4j database and have the data available as Spark DataFrame.

.Read all the nodes with label `Person`
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("labels", "Person")
  .load())

df.show()
----

Visit the xref:reading.adoc[Reading] and xref:writing.adoc[Writing] sections for advanced usage.
