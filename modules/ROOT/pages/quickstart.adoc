= Quickstart
:description: This chapter describes the quick way to get started with Neo4j Connector for Apache Spark.
:page-aliases: python.adoc, playground.adoc, quick-java-example.adoc, aura.adoc, neo4j-cluster.adoc

== Connection

=== Connecting to Neo4j Aura

Connecting to link:https://neo4j.com/docs/aura/[Neo4j Aura] is similar to connecting to on-premise Neo4j instances, but keep in mind:

* Always use a `neo4j+s://` driver URI when communicating with the cluster in the client application.  The optimal
driver URI is provided by AuraDB itself when you create a database.
* In AuraDB Enterprise consider creating a separate username/password for Spark access; avoid running all processes through the default
`neo4j` account.

=== Combining AuraDB and AuraDS

AuraDB is the Neo4j Cloud solution for OLTP processes while AuraDS is designed for computing large-scale graph/ml algorithms in the cloud.
With the Neo4j Spark connector you can easily:

* export the data from AuraDB ingesting it in AuraDS
* run GDS graph/ml algorithms over your data in AuraDS
* write the result of the computation back to AuraDB in order to enrich your transactional data

=== Connecting to a Neo4j Cluster

link:https://neo4j.com/docs/operations-manual/current/clustering/[Neo4j Clustering] is a feature available in
Enterprise Edition, which allows high availability of the database through having multiple database members.

Neo4j Enterprise uses a link:https://neo4j.com/docs/operations-manual/current/clustering/introduction/#causal-clustering-introduction-operational[Primary/Secondary servers]
operational view.
Write operations are always processed by the Primary servers only, while reads can be serviced by either Primary servers (link:https://neo4j.com/docs/operations-manual/current/clustering/internals/#causal-clustering-elections-and-leadership[Leaders or Followers]),
or optionally by Read Replicas, which maintain a copy of the database and serve to scale out read operations
horizontally.

Always use a `neo4j+s://` driver URI when communicating with the cluster in the client application.

== Code examples

You can create Spark DataFrames and write them to your Neo4j database.
Similarly, you can read from your Neo4j database and have the data available as Spark DataFrame.

=== Interactive examples

[.tabbed-example]
====
[.include-with-Scala]
=====

Copy-paste and run this code in the Spark interactive shell.

[source, scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

// Replace with the actual connection URI and credentials
val url = "neo4j://localhost:7687"
val username = "neo4j"
val password = "password"

// Create example Dataset
val df = List(
    ("John", "Doe", 42),
    ("Jane", "Doe", 40)
).toDF("name", "surname", "age")

// Write to Neo4j
df.write.format("org.neo4j.spark.DataSource")
    .mode(SaveMode.Overwrite)
    .option("url", url)
    .option("authentication.basic.username", username)
    .option("authentication.basic.password", password)
    .option("labels", ":Person")
    .option("node.keys", "name,surname")
    .save()
----
=====

[.include-with-Python]
=====

Copy-paste and run this code in the PySpark interactive shell.

For the most part, the API is the same; you should only adopt the syntax for Python by adding backslashes to allow line continuance and avoid running into Python's indentation rules.

[source, python]
----
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Replace with the actual connection URI and credentials
url = "neo4j://localhost:7687"
username = "neo4j"
password = "password"

df = spark.createDataFrame([
    {"name": "John", "surname": "Doe", "age": 32},
    {"name": "Jane", "surname": "Doe", "age": 42}
])

(df.write.format("org.neo4j.spark.DataSource")
    .mode("Overwrite")
    .option("url", url)
    .option("authentication.basic.username", username)
    .option("authentication.basic.password", password)
    .option("labels", ":Person")
    .option("node.keys", "name,surname")
    .save())
----

[NOTE]
======
Some common API constants may need to be referred to as strings in the PySpark API.
For example, the save mode in the Python API is specified with `df.mode("ErrorIfExists")`.
======

To avoid the necessity for backslashes on each line, you can also use parentheses like so:

[source, python]
----
result = (df.write 
  .format("org.neo4j.spark.DataSource")
  .mode("ErrorIfExists")
  .option("url", "neo4j://localhost:7687")
  .option("labels", ":Person")
  .save())
----
=====
====

Visit the xref:reading.adoc[Reading] and xref:writing.adoc[Writing] sections for details on the usage.

=== Java application example

In order to use Neo4j Connector for Apache Spark in your Java application you need to add Spark Packages repository and the dependency.
Look at xref:quickstart.adoc#_installation_guide[this section] for more information.

Let's say you have a Neo4j instance with link:https://neo4j.com/developer/example-data/#built-in-examples[the movie graph] running on `localhost`.

[source, java]
----
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class SparkApp {

    public static void main(String[] args) {
        SparkSession spark = SparkSession
                .builder()
                .appName("Spark SQL Example")
                .config("spark.master", "local")
                .getOrCreate();

        Dataset<Row> ds = spark.read().format("org.neo4j.spark.DataSource")
                .option("url", "neo4j://localhost:7687")
                .option("authentication.basic.username", "neo4j")
                .option("authentication.basic.password", "password")
                .option("labels", "Person")
                .load();

        ds.show();
    }
}
----

== Notebooks

In directory https://github.com/neo4j-contrib/neo4j-spark-connector/tree/5.0/examples[examples] you'll find two useful notebooks that allows you to test Neo4j and PySpark in a cloud-to-cloud environment using Neo4j sandboxes and Google colab.

image::colab-to-sandbox.png[Colab/PySpark to Neo4j sandbox architecture, align="center"]

These notebooks contain a set of examples that explain how the Neo4j Spark connector can fit into your data-driven workflow, and mostly important they allow you to test your knowledge with a set of exercises after each section.

If you have any problem feel free to write a post in the https://community.neo4j.com[Neo4j community forum] or in https://discord.com/invite/neo4j[Discord].

If you want more exercises feel free to open an issue in the https://github.com/neo4j-contrib/neo4j-spark-connector[GitHub repository].

* `neo4j_data_engineering.ipynb` file explains how to interact with the Neo4j Spark connector from a Data Engineering perspective, so how to write your Spark jobs and how to read/write data from/to Neo4j;
* `neo4j_data_science.ipynb` file explains how to interact with the Neo4j Spark connector from a Data Science perspective, so how to combine Pandas (in PySpark) with the Neo4j Graph Data Science library for highlighting frauds in a banking scenario.

=== Test your knowledge

After each session you will find an exercise that will test your knowledge as shown in the image below:

image::exercise-example.png[An exercise, align="center"]

We provide asserts that will test the output of your code and we also provide a solution that you can check by expanding the text `Show a possible solution`
