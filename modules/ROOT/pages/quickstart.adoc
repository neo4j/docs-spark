= Quickstart
:description: This chapter describes the quick way to get started with Neo4j Connector for Apache Spark.
:page-aliases: python.adoc, playground.adoc, quick-java-example.adoc, aura.adoc, neo4j-cluster.adoc

== Before you start

. link:{neo4j-docs-base-uri}/operations-manual/current/installation/[Install] Neo4j or get a link:{neo4j-docs-base-uri}/aura/[Neo4j Aura] instance.
Note down the <<uri, connection URI>> and the access credentials.
+
[TIP]
====
In an Enterprise environment, consider creating a separate username/password for Spark access instead of the default `neo4j` account.
====

. Make sure you have Java (version 8 or above) installed.
. Download Spark as documented on the link:https://spark.apache.org/downloads.html[Spark website].

[#uri]
=== Connection URI

Local instance::
Use the `neo4j://` protocol, for example `neo4j://localhost:7687`.

Neo4j Aura::
Use the `neo4j+s://` protocol.
An Aura connection URI has the form `neo4j+s://xxxxxxxx.databases.neo4j.io`.

Neo4j Cluster::
Use the `neo4j+s://` protocol to link:{neo4j-docs-base-uri}/operations-manual/current/clustering/setup/routing/[route transactions] appropriately (_write_ transactions to the leader, _read_ transactions to followers and read replicas).

[#shell]
== Interactive shell

You can copy-paste and run the following examples directly in the xref:installation.adoc#shell[interactive Spark shell] (`spark-shell` for Scala, `pyspark` for Python).

[.tabbed-example]
====
[.include-with-Scala]
=====

[source, scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

// Replace with the actual connection URI and credentials
val url = "neo4j://localhost:7687"
val username = "neo4j"
val password = "password"

// Create example DataFrame
val df = List(
    ("John", "Doe", 42),
    ("Jane", "Doe", 40)
).toDF("name", "surname", "age")

// Write to Neo4j
df.write.format("org.neo4j.spark.DataSource")
    .mode(SaveMode.Overwrite)
    .option("url", url)
    .option("authentication.basic.username", username)
    .option("authentication.basic.password", password)
    .option("labels", "Person")
    .option("node.keys", "name,surname")
    .save

// Read from Neo4j
spark.read.format("org.neo4j.spark.DataSource")
    .option("url", url)
    .option("authentication.basic.username", username)
    .option("authentication.basic.password", password)
    .option("labels", "Person")
    .load
    .show
----
=====

[.include-with-Python]
=====
Wrap multi-line code in parentheses
[TIP]
======
Wrap chained methods in parentheses to avoid syntax errors.
======

[NOTE]
======
Some common API constants are specified as strings in the PySpark API.
For example, the save mode in the Python API is set with `df.mode("ErrorIfExists")`.
======

[source, python]
----
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Replace with the actual connection URI and credentials
url = "neo4j://localhost:7687"
username = "neo4j"
password = "password"

# Create example DataFrame
df = spark.createDataFrame([
    {"name": "John", "surname": "Doe", "age": 42},
    {"name": "Jane", "surname": "Doe", "age": 40}
])

# Write to Neo4j
(df.write.format("org.neo4j.spark.DataSource")
    .mode("Overwrite")
    .option("url", url)
    .option("authentication.basic.username", username)
    .option("authentication.basic.password", password)
    .option("labels", "Person")
    .option("node.keys", "name,surname")
    .save())

# Read from Neo4j
(spark.read.format("org.neo4j.spark.DataSource")
    .option("url", url)
    .option("authentication.basic.username", username)
    .option("authentication.basic.password", password)
    .option("labels", "Person")
    .load()
    .show())
----
=====
====

[#applications]
== Self-contained applications

To develop a non-Python self-contained application, you need to add the Spark connector to your build as shown in the xref:installation.adoc#applications[Installation] section.
For a Python application, run `spark-submit` directly.

[.tabbed-example]
====
[.include-with-Scala]
=====
. Create a `scala-example` directory.
. Copy the `build.sbt` from the xref:installation.adoc#applications[Installation section] and the `example.jsonl` below into the new directory.
+
.example.jsonl
[source, jsonl]
----
include::example$example.jsonl[]
----
. Create a `src/main/scala` directory and copy the `SparkApp.scala` file below.
+
.SparkApp.scala
[source, scala]
----
include::example$scala/src/main/scala/SparkApp.scala[]
----
. Run `sbt package`.
. Run `spark-submit`:
+
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/spark-submit --packages org.neo4j:neo4j-connector-apache-spark_2.12:{exact-connector-version}_for_spark_3 --class SparkApp target/scala-2.12/spark-app_2.12-1.0.jar
----

=====

[.include-with-Python]
=====

. Create a `python-example` directory.
. Install `pyspark`.
. Copy the `example.jsonl` and `spark_app.py` files below into the new directory.
+
.example.jsonl
[source, jsonl]
----
include::example$example.jsonl[]
----
+
.spark_app.py
[source, python]
----
include::example$python/spark_app.py[]
----
. Run `spark-submit`:
+
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/spark-submit --packages org.neo4j:neo4j-connector-apache-spark_2.12:{exact-connector-version}_for_spark_3 spark_app.py
----

=====

[.include-with-Java]
=====
. Create a `java-example` directory.
. Copy the `pom.xml` from the xref:installation.adoc#applications[Installation section] and the `example.jsonl` below into the new directory.
+
.example.jsonl
[source, jsonl]
----
include::example$example.jsonl[]
----
. Create a `src/main/java` directory and copy the `SparkApp.java` file below.
+
[source, java]
----
include::example$java/src/main/java/SparkApp.java[]
----

. Run `mvn package`.
. Run `spark-submit`:
+
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/spark-submit --packages org.neo4j:neo4j-connector-apache-spark_2.12:{exact-connector-version}_for_spark_3 --class SparkApp target/spark-app-1.0.jar
----

=====
====

== Jupyter notebooks

The code repository includes two https://github.com/neo4j-contrib/neo4j-spark-connector/tree/5.0/examples[Jupyter notebooks] that show how to use the connector in a data-driven workflow:

* `neo4j_data_engineering.ipynb` shows how to create Spark jobs to read data from and write data to Neo4j.
* `neo4j_data_science.ipynb` shows how to combine Pandas (in PySpark) with the Neo4j Graph Data Science library to highlight frauds in a banking scenario.
