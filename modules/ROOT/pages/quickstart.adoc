= Quickstart
:description: This chapter describes the quick way to get started with Neo4j Connector for Apache Spark.
:page-aliases: python.adoc, playground.adoc

You can create Spark DataFrames and write them to your Neo4j database:

.Write the DataFrame to nodes of label `Person`
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val df = List(
  ("John Doe", 32),
  ("Jane Doe", 42),
).toDF("name", "age")

(df.write.format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Append)
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("labels", ":Person")
  .save())
----

Similarly, you can read from your Neo4j database and have the data available as Spark DataFrame.

.Read all the nodes with label `Person`
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("labels", "Person")
  .load())

df.show()
----

Visit the xref:reading.adoc[Reading] and xref:writing.adoc[Writing] sections for advanced usage.

== Using with PySpark / Python

:description: This chapter provides an information on using the Neo4j Connector for Apache Spark with Python.

This connector uses the link:https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-data-source-api-v2.html[DataSource V2 API] in
Spark.

With a properly configured PySpark interpreter, you are able to use Python to call the connector and do all the Spark
work.  

The following examples show what the API looks like in Scala versus Python to aid adaptation of any code examples you might have and to get started quickly.

This first listing is a simple program that reads all `Person` nodes out of a Neo4j instance into a DataFrame, in Scala.

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("labels", "Person:Customer:Confirmed")
  .load()
----

Here is the same program in Python:

[source,python]
----
spark.read.format("org.neo4j.spark.DataSource") \
  .option("url", "bolt://localhost:7687") \
  .option("labels", "Person:Customer:Confirmed") \
  .load()
----

For the most part, the API is the same; you should only adopt the syntax for Python by adding backslashes to allow line continuance and avoid running into Python's indentation rules.

=== API differences

Some common API constants may need to be referred to as strings in the PySpark API. Consider the following two examples in Scala and Python,
focusing on the `SaveMode`.

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person")
  .save()
----

The same program in Python is very similar. Note the language syntax differences and the `mode`:

[source,python]
----
df.write \
  .format("org.neo4j.spark.DataSource") \
  .mode("ErrorIfExists") \
  .option("url", "bolt://localhost:7687") \
  .option("labels", ":Person") \
  .save()
----

To avoid the necessity for backslashes on each line, you can also use parentheses like so:

[source,python]
----
result = (df.write 
  .format("org.neo4j.spark.DataSource")
  .mode("ErrorIfExists")
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person")
  .save())
----

== Playground

In directory https://github.com/neo4j-contrib/neo4j-spark-connector/tree/5.0/examples[examples] you'll find two useful notebooks that allows you to test Neo4j and PySpark in a cloud-to-cloud environment using Neo4j sandboxes and Google colab.

image::colab-to-sandbox.png[Colab/PySpark to Neo4j sandbox architecture, align="center"]

=== The notebooks

These notebooks contain a set of examples that explain how the Neo4j Spark connector can fit into your data-driven workflow, and mostly important they allow you to test your knowledge with a set of exercises after each section.

If you have any problem feel free to write a post in the https://community.neo4j.com[Neo4j community forum] or in https://discord.com/invite/neo4j[Discord].

If you want more exercises feel free to open an issue in the https://github.com/neo4j-contrib/neo4j-spark-connector[GitHub repository].

* `neo4j_data_engineering.ipynb` file explains how to interact with the Neo4j Spark connector from a Data Engineering perspective, so how to write your Spark jobs and how to read/write data from/to Neo4j;
* `neo4j_data_science.ipynb` file explains how to interact with the Neo4j Spark connector from a Data Science perspective, so how to combine Pandas (in PySpark) with the Neo4j Graph Data Science library for highlighting frauds in a banking scenario.

==== Test your knowledge

After each session you will find an exercise that will test your knowledge as shown in the image below:

image::exercise-example.png[An exercise, align="center"]

We provide asserts that will test the output of your code and we also provide a solution that you can check by expanding the text `Show a possible solution`

Enjoy!