= Schema

Spark works with data in a fixed tabular schema.
To accomplish this, the Neo4j Connector has a schema inference system.
It creates the schema based on the data retrieved from the database.
Each read data method has its own strategy to create it, that is explained in the corresponding section.

In general, we first try to use APOC's https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.nodeTypeProperties/[`nodeTypeProperties`]
and https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.relTypeProperties/[`relTypeProperties`] procedures.
If they are not available, we flatten the first `schema.flatten.limit` results and try to infer the schema by the type of each column.

If you don't want this process to happen, set `schema.strategy` to `string` (default is `sample`),
and every column is presented as a string.

[NOTE]
Schema strategy `sample` is good when all instances of a property in Neo4j are of the same type,
and `string` followed by ad-hoc cast is better when property types may differ.
Remember that Neo4j does not enforce property typing, and so `person.age` could for instance sometimes be a `long` and sometimes be a `string`.

== Example

[[sample-strategy]]
.Using sample strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
----

.Dataframe output
|===
|id |name

|0|John Doe
|1|Jane Doe
|===

[[string-strategy]]
.Using string strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .option("schema.strategy", "string")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
|-- id: string (nullable = true)
|-- name: string (nullable = true)
----


.Dataframe output
|===
|id |name

|"0"|"John Doe"
|"1"|"Jane Doe"
|===

As you can see, the Struct returned by the query is made of strings.
To convert *only some* of the values, use regular Scala/Python code:

[source,scala]
----
import scala.jdk.CollectionConverters._
val result = df.collectAsList()
for (row <- result.asScala) {
  // if <some specific condition> then convert like below
  println(s"""Age is: ${row.getString(0).toLong}""")
}
----

[[user-defined-schema]]
=== User defined schema

You can skip the automatic schema extraction process by providing a user defined schema using the `.schema()` method.

.Using user defined schema
[source,scala]
----
import org.apache.spark.sql.types.{DataTypes, StructType, StructField}
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

(spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .schema(StructType(Array(StructField("id", DataTypes.StringType), StructField("name", DataTypes.StringType))))
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .load()
  .show())
----

.Result of the above code
|===
|id |name

|"0"|"John Doe"
|"1"|"Jane Doe"
|===

In this way you have total control over the schema.

[[read-known-problem]]
=== Known problem

Because Neo4j is a schema free database, the following scenario may occur:

[source,cypher]
----
CREATE (p1:Person {age: "32"}), (p2:Person {age: 23})
----

The same field on the same node label has two different types.

Spark doesn't like it since the DataFrame requires a schema,
meaning each column of the DataFrame needs to have its own type.

[source]
----
java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Long
----

In this case you can either clean up and normalize your data, or rely on the connector to
implicitly cast values to `String`.

[NOTE]
This solution is not error-proof, you might still get errors if the values cannot be coerced to String.

When the casting operation happens, this warning appears in your log, letting you know what has happened:

[source]
----
The field "age" has different types: [String, Long]
Every value will be casted to string.
----

The safest solution is to clean your data, but that is not always possible.
This is why `schema.strategy` is introduced, and you can set to `string` to get all the values
converted to string.