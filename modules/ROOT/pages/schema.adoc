= Schema

Spark works with data in a fixed tabular schema.
To accomplish this, the Neo4j Connector has a schema inference system.
It creates the schema based on the data retrieved from the database.
Each read data method has its own strategy to create it, that is explained in the corresponding section.

In general, we first try to use APOC's https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.nodeTypeProperties/[`nodeTypeProperties`]
and https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.relTypeProperties/[`relTypeProperties`] procedures.
If they are not available, we flatten the first `schema.flatten.limit` results and try to infer the schema by the type of each column.

If you don't want this process to happen, set `schema.strategy` to `string` (default is `sample`),
and every column is presented as a string.

[NOTE]
Schema strategy `sample` is good when all instances of a property in Neo4j are of the same type,
and `string` followed by ad-hoc cast is better when property types may differ.
Remember that Neo4j does not enforce property typing, and so `person.age` could for instance sometimes be a `long` and sometimes be a `string`.

== Example

[[sample-strategy]]
.Using sample strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
----

.Dataframe output
|===
|id |name

|0|John Doe
|1|Jane Doe
|===

[[string-strategy]]
.Using string strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .option("schema.strategy", "string")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
|-- id: string (nullable = true)
|-- name: string (nullable = true)
----


.Dataframe output
|===
|id |name

|"0"|"John Doe"
|"1"|"Jane Doe"
|===

As you can see, the Struct returned by the query is made of strings.
To convert *only some* of the values, use regular Scala/Python code:

[source,scala]
----
import scala.jdk.CollectionConverters._
val result = df.collectAsList()
for (row <- result.asScala) {
  // if <some specific condition> then convert like below
  println(s"""Age is: ${row.getString(0).toLong}""")
}
----
