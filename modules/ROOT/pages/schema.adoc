= Schema

Spark works with data in a fixed tabular schema.
To accomplish this, the Neo4j Connector has a schema inference system.
It creates the schema based on the data retrieved from the database.
Each read data method has its own strategy to create it, that is explained in the corresponding section.

In general, we first try to use APOC's https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.nodeTypeProperties/[`nodeTypeProperties`]
and https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.relTypeProperties/[`relTypeProperties`] procedures.
If they are not available, we flatten the first `schema.flatten.limit` results and try to infer the schema by the type of each column.

If you don't want this process to happen, set `schema.strategy` to `string` (default is `sample`),
and every column is presented as a string.

[NOTE]
Schema strategy `sample` is good when all instances of a property in Neo4j are of the same type,
and `string` followed by ad-hoc cast is better when property types may differ.
Remember that Neo4j does not enforce property typing, and so `person.age` could for instance sometimes be a `long` and sometimes be a `string`.

== Example

[[sample-strategy]]
.Using sample strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "neo4j://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
----

.Dataframe output
|===
|id |name

|0|John Doe
|1|Jane Doe
|===

[[string-strategy]]
.Using string strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "neo4j://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .option("schema.strategy", "string")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
|-- id: string (nullable = true)
|-- name: string (nullable = true)
----


.Dataframe output
|===
|id |name

|"0"|"John Doe"
|"1"|"Jane Doe"
|===

As you can see, the Struct returned by the query is made of strings.
To convert *only some* of the values, use regular Scala/Python code:

[source,scala]
----
import scala.jdk.CollectionConverters._
val result = df.collectAsList()
for (row <- result.asScala) {
  // if <some specific condition> then convert like below
  println(s"""Age is: ${row.getString(0).toLong}""")
}
----

== Schema considerations

Neo4j does not have a fixed schema; individual properties can contain multiple differently typed values. Spark
on the other hand tends to expect a fixed schema. For this reason, the connector contains a number of schema
inference techniques that help ease this mapping. Paying close attention to how these features work can 
explain different scenarios.

The two core techniques are:

* <<APOC>>
* <<Automatic sampling>>

=== APOC

If your Neo4j installation has APOC installed, this approach is used by default. The stored procedures within APOC allow inspection of the
metadata in your graph and provide information such as the type of relationship properties and the universe of possible properties attached to a given node label.

You may try these calls by yourself on your Neo4j database if you wish, simply execute:

```cypher
CALL apoc.meta.nodeTypeProperties();
CALL apoc.meta.relTypeProperties();
```

Inspect the results.  These results are how the Neo4j Connector for Apache Spark represents the metadata of nodes and relationships read into DataFrames.

This approach uses a configurable sampling technique that looks through many (but not all) instances in the database to build a profile of the valid
values that exist within properties.  If the schema that is produced is not what is expected, take care to inspect the underlying data to ensure it has a consistent
property set across all nodes of a label, or investigate tuning the sampling approach.

==== Tune parameters

You can tune the configuration parameters of the https://neo4j.com/labs/apoc/4.1/database-introspection/meta/[two APOC procedures]
via the `option` method as it follows:

```scala
ss.read
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", "Product")
      .option("apoc.meta.nodeTypeProperties", """{"sample": 10}""")
      .load
```

or

```scala
ss.read
      .format(classOf[DataSource].getName)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("relationship", "BOUGHT")
      .option("relationship.source.labels", "Product")
      .option("relationship.target.labels", "Person")
      .option("apoc.meta.relTypeProperties", """{"sample": 10}""")
      .load
```

For both procedures you can pass all the supported parameters except for:

* `includeLabels` for `apoc.meta.nodeTypeProperties`, because you use the labels defined in
the `labels` option.
* `includeRels` for `apoc.meta.relTypeProperties`, because you use the one defined in
the `relationship` option.

===== Fine tuning

As these two procedures sample the graph to extract the metadata necessary for building the <<Tables for labels>>,
in most real-world scenarios, it is crucial to tune the sampling parameters properly because using of them
can be expensive and impact the performance of your extraction job.

=== Automatic sampling

In some installations and environments, the key APOC calls above are not available.
In these cases, the connector automatically samples the first few records and infers
the correct data type from the examples that it sees.

[NOTE]
**Automatic sampling may be error prone and may produce incorrect results,
particularly in cases where a single Neo4j property exists with several different data types.
Consistent typing of properties is strongly recommended.**