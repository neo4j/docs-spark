= Writing to Neo4j

:description: The chapters describes writing methods to a Neo4j database using Neo4j Spark connector.
:table-header-code: Code
:table-header-cypher: Equivalent Cypher query

The following section covers the DataSource Writer and how to transfer the Spark dataset content into Neo4j.
The connector provides batch writes to speed up the ingestion process, so if the process at some point fails, all the previous data are already persisted.

There are two main approaches for writing data to a Neo4j database with the connector:

* <<write-node, Write nodes>> using the `labels` option.
* <<write-rel, Write a relationship>> with source and target nodes using the `relationship` option.

If you need more flexibility, you can also run a xref:write/query.adoc[custom Cypher query] using the `query` option.

[example]
== Example

The following code inserts 10 nodes into Neo4j.
Each node has two labels (`Person` and `Customer`) and four properties (`name`, `surname`, `age`, and `livesIn`).

.Write example
[source, scala, role=nocollapse]
----
include::example$scala/WriteBasic.scala[tags=!setup]
----

[[save-mode]]
== Save mode

The connector supports two save modes:

* The `Append` mode builds a `CREATE` query.
* The `Overwrite` mode builds a `MERGE` query (requires the `node.keys` option).

== Type mapping

The type mapping between Spark DataFrames and Neo4j is summarized in the xref:types.adoc[] section.

[[write-nodes]]
== Write nodes

With the `labels` option, the connector writes a DataFrame to the Neo4j database as a set of nodes with the given labels.

The connector builds a `CREATE` or a `MERGE` Cypher query (depending on the <<save-mode, SaveMode>>) that uses the `UNWIND` clause to write a batch of rows (an `events` list with size defined by the `batch.size` option).

The code from the <<example, example>> results in a Cypher query like the following:

[cols="1,1]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Append)
  .option("labels", ":Person:Customer")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
CREATE (n:Person:Customer)
SET n += event.properties
----
|===

With the `Overwrite` mode, you must specify the DataFrame columns to use as keys to match the nodes.
The `node.keys` option takes a comma-separated list of `key:value` pairs, where the key is the DataFrame column name and the value is the node property name.

[NOTE]
====
If `key` and `value` are the same, you can omit the `value`.
For example, `"name:name,surname:surname"` is equivalent to `"name,surname"`.
====

The same code using the `Overwrite` save mode results in a Cypher query like the following:

[cols="1,1]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "name,surname")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
MERGE (n:Person:Customer {
  name: event.keys.name, 
  surname: event.keys.surname
})
SET n += event.properties
----
|===

[[write-rel]]
== Write a relationship

With the `relationship` option, the connector writes a DataFrame to the Neo4j database by specifying source, target nodes, and a relationship.

[WARNING]
====
To avoid deadlocks, always use a single partition (for example with `coalesce(1)`) before writing relationships to Neo4j.
====

The connector builds a Cypher query like the following:

[source, cypher]
----
UNWIND $events AS event
CREATE (source:Person)
SET source = event.source
CREATE (target:Product)
SET target = event.target
CREATE (source)-[rel:BOUGHT]->(target)
SET rel += event.rel
----

The way the actual query is built depends on a number of options.

* `mode` defines whether the query uses a `CREATE` or a `MERGE` clause (see <<save-mode, save mode>>).
* `relationship.save.strategy` defines the <<strategies, save strategy>> to use:
** The `native` strategy requires the DataFrame to use a specific schema.
** The `keys` strategy is more flexible.
* `relationship.{source,target}.save.mode` defines the way nodes are used, and can be set independently for source and target nodes:
** `Match` mode performs a `MATCH`.
** `Append` mode performs a `CREATE`.
** `Overwrite` mode performs a `MERGE`.
* `relationship.{source,target}.labels` define the labels to assign to source and target nodes.
* `relationship.{source,target}.node.keys`:
When using `Match` or `Overwrite` as the `relationship.{source,target}.save.mode`, you need to specify keys that identify the nodes.

Additionally, if the `relationship.save.strategy` is `keys`:

* `relationship.properties` defines which DataFrame columns to write as relationship properties.
* `relationship.{source,target}.node.properties` is a comma-separated list of `key:value` pairs that maps DataFrame columns to node properties.

[#strategies]
=== Save strategies

[[strategy-native]]
==== `native` strategy

The `native` strategy is useful when the DataFrame schema conforms to the <<reading.adoc#rel-schema-columns,Relationship read schema>> with the `relationship.nodes.map` option set to `false`.
This means that the DataFrame must include at least one of the `rel.[property name]`, `source.[property name]`, or `target.[property name]` columns.

[NOTE]
====
If the provided DataFrame schema doesn't conform to the required schema, meaning that none of the required columns is present,
the write fails.
====

A good use case for this mode is transferring data from a database to another one.
In fact, using the connector to xref:reading.adoc#read-rel[read a relationship] first, the resulting DataFrame has already the correct schema.

[NOTE]
====
The default save mode for source and target nodes is `Match`.
That means that the relationship can be created only if the nodes are already in your database.
====

.Read + filter + write
[source, scala, role=nocollapse]
----
val originalDf = spark.read.format("org.neo4j.spark.DataSource")
  .option("relationship", "PLAYS")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Musician")
  .option("relationship.target.labels", "Instrument")
  .load()

originalDf
    .where("`rel.experience` > 15")
    .write
    .mode("Append")
    .format("org.neo4j.spark.DataSource")
    .option("relationship", "PLAYS_WELL")
    .option("relationship.source.labels", ":ExperiencedMusician")
    .option("relationship.source.save.mode", "Append")
    .option("relationship.target.labels", "Instrument")
    .option("relationship.target.save.mode", "Append")
    .save()
----

The code in the following example creates:

* `:Musician` nodes with a `name` property
* `:Instrument` nodes with a `name` property
* `:PLAYS` relationships between `:Musician` and `:Instrument` nodes, with an `experience` property

.Native strategy, `Append` node save mode
[cols="1,1"]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala, role=nocollapse]
----
// Columns representing node/relationships properties
// must use the "rel.", "source.", or "target." prefix
val df = Seq(
  (12, "John Bonham", "Drums"),
  (19, "John Mayer", "Guitar"),
  (32, "John Scofield", "Guitar"),
  (15, "John Butler", "Guitar")
).toDF("rel.experience", "source.name", "target.name")

df
  .write
  // Create new relationships
  .mode("Append")
  .format("org.neo4j.spark.DataSource")
  // Assign a type to the relationships
  .option("relationship", "PLAYS")
  // Create source nodes and assign them a label
  .option("relationship.source.save.mode", "Append")
  .option("relationship.source.labels", ":Musician")
  // Create target nodes and assign them a label
  .option("relationship.target.save.mode", "Append")
  .option("relationship.target.labels", ":Instrument")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
CREATE (source:Musician)
SET source += event.source.properties
CREATE (target:Instrument)
SET target += event.target.properties
CREATE (source)-[rel:PLAYS]->(target)
SET rel += event.rel.properties
----
|===

The same example, using `Overwrite` as a save mode for nodes and the `node.keys` option:

.Native strategy, `Overwrite` node save mode
[cols="1,1"]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala, role=nocollapse]
----
// Columns representing node/relationships properties
// must use the "rel.", "source.", or "target." prefix
val df = Seq(
  (12, "John Bonham", "Drums"),
  (19, "John Mayer", "Guitar"),
  (32, "John Scofield", "Guitar"),
  (15, "John Butler", "Guitar")
).toDF("rel.experience", "source.name", "target.name")

df
  .write
  // Create new relationships
  .mode("Append")
  .format("org.neo4j.spark.DataSource")
  // Assign a type to the relationships
  .option("relationship", "PLAYS")
  // Overwrite source nodes and assign them a label
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.labels", ":Musician")
  // Node keys are mandatory for overwrite save mode
  .option("relationship.source.node.keys", "source.name:name")
  // Overwrite target nodes and assign them a label
  .option("relationship.target.save.mode", "Overwrite")
  .option("relationship.target.labels", ":Instrument")
  // Node keys are mandatory for overwrite save mode
  .option("relationship.target.node.keys", "target.name:name")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
MERGE (source:Musician {name: event.source.keys.name})
SET source += event.source.properties
MERGE (target:Instrument {name: event.target.keys.name})
SET target += event.target.properties
CREATE (source)-[rel:PLAYS]->(target)
SET rel += event.rel.properties
----
|===

[[strategy-keys]]
==== `keys` strategy

The `keys` strategy gives more control on how relationships and nodes are written.
It does not require any specific schema for the DataFrame.

As in the case of using the `native` strategy, you can specify node keys to identify nodes for the `Match` and `Overwrite` mode.
In addition, you can also specify which columns to write as node and relationship properties.

[[rel-specify-keys]]
.Specify node and relationships keys
[cols="1,1"]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala, role=nocollapse]
----
val df = Seq(
        (12, "John Bonham", "Drums"),
        (19, "John Mayer", "Guitar"),
        (32, "John Scofield", "Guitar"),
        (15, "John Butler", "Guitar")
    ).toDF("experience", "name", "instrument")

df
  .write
  // Create new relationships
  .mode("Append")
  .format("org.neo4j.spark.DataSource")
  // Assign a type to the relationships
  .option("relationship", "PLAYS")
  // Use `keys` strategy
  .option("relationship.save.strategy", "keys")
  .option("relationship.properties", "experience:experience")
  // Create source nodes and assign them a label
  .option("relationship.source.save.mode", "Append")
  .option("relationship.source.labels", ":Musician")
  // Map the DataFrame columns to node properties
  .option("relationship.source.node.properties", "name:name")
  // Create target nodes and assign them a label
  .option("relationship.target.save.mode", "Append")
  .option("relationship.target.labels", ":Instrument")
  // Map the DataFrame columns to node properties
  .option("relationship.target.node.properties", "instrument:name")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
CREATE (source:Musician)
SET source += event.source.properties
CREATE (target:Instrument)
SET target += event.target.properties
CREATE (source)-[rel:PLAYS]->(target)
SET rel += event.rel.properties
----
|===

[NOTE]
If `key` and `value` are the same field you can specify one without the colon.
For example, if you have `.option("relationship.source.node.properties", "name:name,email:email")`, you can also write
`.option("relationship.source.node.properties", "name,email")`.
Same applies for `relationship.source.node.keys` and `relationship.target.node.keys`.

[[node-save-strategies]]
=== Node save strategies

[NOTE]
====
For `Overwrite` mode you *must have unique constraints on the keys*.
====

== Performance considerations

Since writing is typically an expensive operation, make sure you write only the columns you need from the DataFrame.
For example, if the columns from the data source are `name`, `surname`, `age`, and `livesIn`, but you only need `name` and `surname`, you can do the following:

[source, scala]
----
ds.select(ds("name"), ds("surname"))
  .write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "neo4j://localhost:7687")
  .option("labels", ":Person:Customer")
  .save()
----
