= Writing to Neo4j

:description: The chapters describes writing methods to a Neo4j database using Neo4j Spark connector.
:table-header-code: Code
:table-header-cypher: Equivalent Cypher query

The connector provides three options to write data to a Neo4j database.

.Write options
[cols="1, 2, 2, 1"]
|===
|Option|Description|Value|Default

|`labels`
|Use this if you only need to <<write-nodes, create or update nodes>> with their properties, or as a first step before adding relationships.
|Colon-separated list of node labels to create or update.
|_(empty)_

|`relationship`
|Use this if you need to <<write-rel, create or update relationships>> along with their source and target nodes.
|Relationship type to create or update.
|_(empty)_

|`query`
|Use this if you need more flexibility and know how to xref:write/query.adoc[write a Cypher query].
|Cypher query with a `CREATE` or `MERGE` clause.
|_(empty)_
|===

[#save-mode]
== Save mode

Regardless of the write option, the connector supports two save modes:

* The `Append` mode creates new nodes or relationships by building a `CREATE` Cypher query.
* The `Overwrite` mode creates or updates new nodes or relationships by building a `MERGE` Cypher query.
** Requires the `node.keys` option when used with the `labels` option.
** Requires the `relationship.source.node.keys` and `relationship.target.node.keys` when used with the `relationship` option.

== Type mapping

The type mapping between Spark DataFrames and Neo4j is summarized in the xref:types.adoc[] section.

[#example]
== Basic example

The following code inserts 10 nodes into Neo4j.
Each node has two labels (`Person` and `Customer`) and four properties (`name`, `surname`, `age`, and `livesIn`).

.Write example
[source, scala, role=nocollapse]
----
include::example$scala/WriteBasic.scala[tags=!setup]
----

[#write-nodes]
== Write nodes

With the `labels` option, the connector writes a DataFrame to the Neo4j database as a set of nodes with the given labels.

The connector builds a `CREATE` or a `MERGE` Cypher query (depending on the <<save-mode, SaveMode>>) that uses the `UNWIND` clause to write a batch of rows (an `events` list with size defined by the `batch.size` option).

The code from the <<example, example>> results in a Cypher query like the following:

[cols="1,1]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Append)
  .option("labels", ":Person:Customer")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
CREATE (n:Person:Customer)
SET n += event.properties
----
|===

With the `Overwrite` mode, you must specify the DataFrame columns to use as keys to match the nodes.
The `node.keys` option takes a comma-separated list of `key:value` pairs, where the key is the DataFrame column name and the value is the node property name.

[NOTE]
====
If `key` and `value` are the same, you can omit the `value`.
For example, `"name:name,surname:surname"` is equivalent to `"name,surname"`.
====

The same code using the `Overwrite` save mode results in a Cypher query like the following:

[cols="1,1]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala]
----
df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("labels", ":Person:Customer")
  .option("node.keys", "name,surname")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
MERGE (n:Person:Customer {
  name: event.keys.name, 
  surname: event.keys.surname
})
SET n += event.properties
----
|===

[#write-rel]
== Write a relationship

With the `relationship` option, the connector writes a DataFrame to the Neo4j database by specifying source, target nodes, and a relationship.

[WARNING]
====
To avoid deadlocks, always use a single partition (for example with `coalesce(1)`) before writing relationships to Neo4j.
====

The connector builds a Cypher query like the following:

[source, cypher]
----
UNWIND $events AS event
CREATE (source:Person)
SET source = event.source
CREATE (target:Product)
SET target = event.target
CREATE (source)-[rel:BOUGHT]->(target)
SET rel += event.rel
----

The `mode` defines whether the query uses a `CREATE` or a `MERGE` clause (see <<save-mode, save mode>>).

The way the actual query is built depends on a number of options.

[cols="4, 3a, 3a, 1"]
|===
|Option |Description |Value |Default

|`relationship.save.strategy`
|Defines the <<strategies, save strategy>> to use.
|
* `native` requires the DataFrame to use a specific schema.
* `keys` is more flexible.
|`native`

|`relationship.source.save.mode`

and

`relationship.target.save.mode`
|Define the Source <<node-save-strategies, node save mode>>, and can be set independently for source and target nodes.
|
* `Match` mode performs a `MATCH`.
* `Append` mode performs a `CREATE`.
* `Overwrite` mode performs a `MERGE`.
|`Match`

|`relationship.source.labels`

and

`relationship.target.labels`
|*Required.*

Define the labels to assign to source and target nodes.
|Colon-separated list of labels.
|_(empty)_

|`relationship.source.node.keys`

and

`relationship.target.node.keys`
|When the <<node-save-strategies, node save mode>> is `Match` or `Overwrite`, defines keys that identify the nodes.
|Comma-separated list of key-value pairs.
|_(empty)_

|`relationship.properties`
|When the <<strategies, save strategy>> is `keys`, defines which DataFrame columns to write as relationship properties.
|Comma-separated list of `key:value` pairs that map DataFrame columns to relationship properties.
|_(empty)_

|`relationship.source.node.properties`

and

`relationship.target.node.properties`
|When the <<strategies, save strategy>> is `keys`, defines which DataFrame columns to write as source/target properties.
|Comma-separated list of `key:value` pairs that map DataFrame columns to node properties.
|_(empty)_
|===

[#strategies]
=== Save strategies

[#strategy-native]
==== `native` strategy

The `native` strategy is useful when the DataFrame schema conforms to the <<reading.adoc#rel-schema-columns,Relationship read schema>> with the `relationship.nodes.map` option set to `false`.
This means that the DataFrame must include at least one of the `rel.[property name]`, `source.[property name]`, or `target.[property name]` columns.

[NOTE]
====
If the provided DataFrame schema doesn't conform to the required schema, meaning that none of the required columns is present,
the write fails.
====

A good use case for this mode is transferring data from a database to another one.
In fact, using the connector to xref:reading.adoc#read-rel[read a relationship] first, the resulting DataFrame has already the correct schema.

[NOTE]
====
The default save mode for source and target nodes is `Match`.
That means that the relationship can be created only if the nodes are already in your database.
====

.Read + filter + write
[source, scala, role=nocollapse]
----
val originalDf = spark.read.format("org.neo4j.spark.DataSource")
  .option("relationship", "PLAYS")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Musician")
  .option("relationship.target.labels", "Instrument")
  .load()

originalDf
    .where("`rel.experience` > 15")
    .write
    .mode("Append")
    .format("org.neo4j.spark.DataSource")
    .option("relationship", "PLAYS_WELL")
    .option("relationship.source.labels", ":ExperiencedMusician")
    .option("relationship.source.save.mode", "Append")
    .option("relationship.target.labels", "Instrument")
    .option("relationship.target.save.mode", "Append")
    .save()
----

The code in the following example creates:

* `:Musician` nodes with a `name` property
* `:Instrument` nodes with a `name` property
* `:PLAYS` relationships between `:Musician` and `:Instrument` nodes, with an `experience` property

.Native strategy, `Append` node save mode
[cols="1,1"]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala, role=nocollapse]
----
// Columns representing node/relationships properties
// must use the "rel.", "source.", or "target." prefix
val df = Seq(
  (12, "John Bonham", "Drums"),
  (19, "John Mayer", "Guitar"),
  (32, "John Scofield", "Guitar"),
  (15, "John Butler", "Guitar")
).toDF("rel.experience", "source.name", "target.name")

df
  .write
  // Create new relationships
  .mode("Append")
  .format("org.neo4j.spark.DataSource")
  // Assign a type to the relationships
  .option("relationship", "PLAYS")
  // Create source nodes and assign them a label
  .option("relationship.source.save.mode", "Append")
  .option("relationship.source.labels", ":Musician")
  // Create target nodes and assign them a label
  .option("relationship.target.save.mode", "Append")
  .option("relationship.target.labels", ":Instrument")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
CREATE (source:Musician)
SET source += event.source.properties
CREATE (target:Instrument)
SET target += event.target.properties
CREATE (source)-[rel:PLAYS]->(target)
SET rel += event.rel.properties
----
|===

The same example, using `Overwrite` as a save mode for nodes and the `node.keys` option:

.Native strategy, `Overwrite` node save mode
[cols="1,1"]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala, role=nocollapse]
----
// Columns representing node/relationships properties
// must use the "rel.", "source.", or "target." prefix
val df = Seq(
  (12, "John Bonham", "Drums"),
  (19, "John Mayer", "Guitar"),
  (32, "John Scofield", "Guitar"),
  (15, "John Butler", "Guitar")
).toDF("rel.experience", "source.name", "target.name")

df
  .write
  // Create new relationships
  .mode("Append")
  .format("org.neo4j.spark.DataSource")
  // Assign a type to the relationships
  .option("relationship", "PLAYS")
  // Overwrite source nodes and assign them a label
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.labels", ":Musician")
  // Node keys are mandatory for overwrite save mode
  .option("relationship.source.node.keys", "source.name:name")
  // Overwrite target nodes and assign them a label
  .option("relationship.target.save.mode", "Overwrite")
  .option("relationship.target.labels", ":Instrument")
  // Node keys are mandatory for overwrite save mode
  .option("relationship.target.node.keys", "target.name:name")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
MERGE (source:Musician {name: event.source.keys.name})
SET source += event.source.properties
MERGE (target:Instrument {name: event.target.keys.name})
SET target += event.target.properties
CREATE (source)-[rel:PLAYS]->(target)
SET rel += event.rel.properties
----
|===

[#strategy-keys]
==== `keys` strategy

The `keys` strategy gives more control on how relationships and nodes are written.
It does not require any specific schema for the DataFrame.

As in the case of using the `native` strategy, you can specify node keys to identify nodes for the `Match` and `Overwrite` mode.
In addition, you can also specify which columns to write as node and relationship properties.

[#rel-specify-keys]
.Specify node and relationships keys
[cols="1,1"]
|===
|{table-header-code}|{table-header-cypher}

a|
[source, scala, role=nocollapse]
----
val df = Seq(
        (12, "John Bonham", "Drums"),
        (19, "John Mayer", "Guitar"),
        (32, "John Scofield", "Guitar"),
        (15, "John Butler", "Guitar")
    ).toDF("experience", "name", "instrument")

df
  .write
  // Create new relationships
  .mode("Append")
  .format("org.neo4j.spark.DataSource")
  // Assign a type to the relationships
  .option("relationship", "PLAYS")
  // Use `keys` strategy
  .option("relationship.save.strategy", "keys")
  .option("relationship.properties", "experience:experience")
  // Create source nodes and assign them a label
  .option("relationship.source.save.mode", "Append")
  .option("relationship.source.labels", ":Musician")
  // Map the DataFrame columns to node properties
  .option("relationship.source.node.properties", "name:name")
  // Create target nodes and assign them a label
  .option("relationship.target.save.mode", "Append")
  .option("relationship.target.labels", ":Instrument")
  // Map the DataFrame columns to node properties
  .option("relationship.target.node.properties", "instrument:name")
  .save()
----

a|
[source, cypher]
----
UNWIND $events AS event
CREATE (source:Musician)
SET source += event.source.properties
CREATE (target:Instrument)
SET target += event.target.properties
CREATE (source)-[rel:PLAYS]->(target)
SET rel += event.rel.properties
----
|===

[NOTE]
If `key` and `value` are the same field you can specify one without the colon.
For example, if you have `.option("relationship.source.node.properties", "name:name,email:email")`, you can also write
`.option("relationship.source.node.properties", "name,email")`.
Same applies for `relationship.source.node.keys` and `relationship.target.node.keys`.

[#node-save-strategies]
=== Node save strategies

[NOTE]
====
For `Overwrite` mode you *must have unique constraints on the keys*.
====

== Performance considerations

Since writing is typically an expensive operation, make sure you write only the columns you need from the DataFrame.
For example, if the columns from the data source are `name`, `surname`, `age`, and `livesIn`, but you only need `name` and `surname`, you can do the following:

[source, scala]
----
ds.select(ds("name"), ds("surname"))
  .write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "neo4j://localhost:7687")
  .option("labels", ":Person:Customer")
  .save()
----
