= Read a relationship

include::partial$sparksession.adoc[]

You can read a relationship and its source and target nodes by specifying the relationship type, the source node labels, and the target node labels.

[source, scala]
----
val df = spark.read
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "BOUGHT")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

df.show()
----

The code above creates the following Cypher query:

[source, cypher]
----
MATCH (source:Person)-[rel:BOUGHT]->(target:Product)
RETURN source, rel, target
----

[[rel-schema-columns]]
== DataFrame columns

When reading data with this method, the DataFrame contains the following columns:

* `<rel.id>`: internal Neo4j ID
* `<rel.type>`: relationship type
* `rel.[property name]`: relationship properties

Additional columns are added depending on the value of the `relationship.nodes.map` option:

|===
|`relationship.nodes.map` set to `false` (default)|`relationship.nodes.map` set to `true`

a|
* `<source.id>`: internal Neo4j ID of source node
* `<source.labels>`: list of labels for source node
* `<target.id>`: internal Neo4j ID of target node
* `<target.labels>`: list of labels for target node
* `source.[property name]`: source node properties
* `target.[property name]`: target node properties

a|
* `source`: map of source node properties
* `target`: map of target node properties
|===

Examples:

[[rel-schema-no-map]]
.`relationship.nodes.map` set to `false`
[source, scala]
----
val df = spark.read
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

df.show()
----

.Result
|===
|<rel.id>|<rel.type>|<source.id>|<source.labels>|source.surname|source.name|source.id|<target.id>|<target.labels>|target.name|rel.order|rel.quantity

|3189|BOUGHT|1100|[Person]|Doe|John|1|1040|[Product]|Product1|ABC100|200
|3190|BOUGHT|1099|[Person]|Doe|Jane|2|1039|[Product]|Product1|ABC200|100
|===

.`relationship.nodes.map` set to `true`
[source, scala]
----
val df = spark.read
  .format("org.neo4j.spark.DataSource")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "true")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()
  
// Use `false` to print the whole DataFrame
df.show(false)
----

.Result
[cols="2,2,3l,3l,2,2"]
|===
|<rel.id>|<rel.type>|<source>|<target>|rel.order|rel.quantity

|3189|BOUGHT|{surname: "Doe", name: "John", id: 1, <labels>: ["Person"], <id>: 1100}|{name: "Product 1", <labels>: ["Product"], <id>: 1040}|ABC100|200
|3190|BOUGHT|{surname: "Doe", name: "Jane", id: 2, <labels>: ["Person"], <id>: 1099}|{name: "Product 1", <labels>: ["Product"], <id>: 1039}|ABC200|100
|===

== Schema

If the link:{neo4j-docs-base-uri}/apoc/current/[APOC library] is available on the Neo4j instance, the schema is retrieved via the link:{neo4j-docs-base-uri}/apoc/current/overview/apoc.meta/apoc.meta.relTypeProperties/[`apoc.meta.relTypeProperties`^] procedure.
Otherwise, the following Cypher query is executed:

[source, cypher]
----
MATCH (source:<source_labels>)-[rel:<relationship>]->(target:<target_labels>)  <1> <2> <3>
RETURN rel
ORDER BY rand()
LIMIT <limit> <4>
----
<1> `<source_labels>` is the list of labels provided by `relationship.source.labels` option.
<2> `<target_labels>` is the list of labels provided by `relationship.target.labels` option.
<3> `<relationship>` is the list of labels provided by `relationship` option.
<4> `<limit>` is the value provided via `schema.flatten.limit`.

== Filtering

You can use the `where` and `filter` functions in Spark to filter properties of the relationship, the source node, or the target node.
The correct format of the filter depends on the value of `relationship.nodes.map` option.

|===
|`relationship.nodes.map` set to `false` (default)|`relationship.nodes.map` set to `true`

a|
* ``\`source.[property]` `` for the source node properties
* ``\`rel.[property]` `` for the relationship property
* ``\`target.[property]` `` for the target node property

a|
* ``\`<source>`.\`[property]` `` for the source node map properties
* ``\`<rel>`.\`[property]` `` for the relationship map property
* ``\`<target>`.\`[property]` `` for the target node map property
|===

Examples:

.`relationship.nodes.map` set to `false`
[source, scala]
----
val df = spark.read.format("org.neo4j.spark.DataSource")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

df.where("`source.id` > 1").show()
----

.Result
|===
|<rel.id>|<rel.type>|<source.id>|<source.labels>|source.surname|source.name|<target.id>|<target.labels>|target.name|rel.order|rel.quantity

|3190|BOUGHT|1099|[Person]|Doe|Jane|2|1039|[Product]|Product1|ABC200|100
|===

.`relationship.nodes.map` set to `true`
[source, scala]
----
val df = spark.read.format("org.neo4j.spark.DataSource")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "true")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

// Use `false` to print the whole DataFrame
df.where("`<source>`.`id` > 1").show(false)
----

.Result
[cols="2,2,3l,3l,2,2"]
|===
|<rel.id>|<rel.type>|<source>|<target>|rel.order|rel.quantity

|3190|BOUGHT|{surname: "Doe", name: "Jane", id: 2, <labels>: ["Person"], <id>: 1099}|{name: "Product 1", <labels>: ["Product"], <id>: 1039}|ABC200|100
|===