= Inferred schema

Spark works with data in a fixed tabular schema.
To accomplish this, the Neo4j Connector has a schema inference system.
It creates the schema based on the data retrieved from the database.
Each read data method has its own strategy to create it, that is explained in the corresponding section.

In general, we first try to use APOC's https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.nodeTypeProperties/[`nodeTypeProperties`]
and https://neo4j.com/labs/apoc/4.4/overview/apoc.meta/apoc.meta.relTypeProperties/[`relTypeProperties`] procedures.
If they are not available, we flatten the first `schema.flatten.limit` results and try to infer the schema by the type of each column.

If you don't want this process to happen, set `schema.strategy` to `string` (default is `sample`),
and every column is presented as a string.

[NOTE]
Schema strategy `sample` is good when all instances of a property in Neo4j are of the same type,
and `string` followed by ad-hoc cast is better when property types may differ.
Remember that Neo4j does not enforce property typing, and so `person.age` could for instance sometimes be a `long` and sometimes be a `string`.

== Schema considerations

Neo4j does not have a fixed schema; individual properties can contain multiple differently typed values. Spark
on the other hand tends to expect a fixed schema. For this reason, the connector contains a number of schema
inference techniques that help ease this mapping. Paying close attention to how these features work can 
explain different scenarios.

The two core techniques are:

* <<APOC>>
* <<Automatic sampling>>

=== APOC

If your Neo4j installation has APOC installed, this approach is used by default. The stored procedures within APOC allow inspection of the
metadata in your graph and provide information such as the type of relationship properties and the universe of possible properties attached to a given node label.

You may try these calls by yourself on your Neo4j database if you wish, simply execute:

```cypher
CALL apoc.meta.nodeTypeProperties();
CALL apoc.meta.relTypeProperties();
```

Inspect the results.  These results are how the Neo4j Connector for Apache Spark represents the metadata of nodes and relationships read into DataFrames.

This approach uses a configurable sampling technique that looks through many (but not all) instances in the database to build a profile of the valid
values that exist within properties.  If the schema that is produced is not what is expected, take care to inspect the underlying data to ensure it has a consistent
property set across all nodes of a label, or investigate tuning the sampling approach.

=== Automatic sampling

In some installations and environments, the key APOC calls above are not available.
In these cases, the connector automatically samples the first few records and infers
the correct data type from the examples that it sees.

[NOTE]
**Automatic sampling may be error prone and may produce incorrect results,
particularly in cases where a single Neo4j property exists with several different data types.
Consistent typing of properties is strongly recommended.**

== Inferred schema for `labels`

If the link:{neo4j-docs-base-uri}/apoc/current/[APOC library] is available on the Neo4j instance, the schema is retrieved via the link:{neo4j-docs-base-uri}/apoc/current/overview/apoc.meta/apoc.meta.nodeTypeProperties/[`apoc.meta.nodeTypeProperties`^] procedure.
Otherwise, the following Cypher query is executed:

[source, cypher]
----
MATCH (n:<labels>) <1>
RETURN n
ORDER BY rand()
LIMIT <limit> <2>
----
<1> `<labels>` is the list of labels provided by the `labels` option.
<2> `<limit>` is the value provided by the `schema.flatten.limit` option.

The schema is recreated from the flattened query result.

== Inferred schema for `relationships`

If the link:{neo4j-docs-base-uri}/apoc/current/[APOC library] is available on the Neo4j instance, the schema is retrieved via the link:{neo4j-docs-base-uri}/apoc/current/overview/apoc.meta/apoc.meta.relTypeProperties/[`apoc.meta.relTypeProperties`^] procedure.
Otherwise, the following Cypher query is executed:

[source, cypher]
----
MATCH (source:<source_labels>)-[rel:<relationship>]->(target:<target_labels>)  <1> <2> <3>
RETURN rel
ORDER BY rand()
LIMIT <limit> <4>
----
<1> `<source_labels>` is the list of labels provided by `relationship.source.labels` option.
<2> `<target_labels>` is the list of labels provided by `relationship.target.labels` option.
<3> `<relationship>` is the list of labels provided by `relationship` option.
<4> `<limit>` is the value provided via `schema.flatten.limit`.

== Inferred schema for `query`

[[sample-strategy]]
.Using sample strategy
[source,scala]
----
val df = spark.read
  .format("org.neo4j.spark.DataSource")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .load()

df.printSchema()
df.show()
----

.Schema output
----
root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
----

.Dataframe output
|===
|id |name

|0|John Doe
|1|Jane Doe
|===

[[string-strategy]]
.Using string strategy
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = (spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "neo4j://localhost:7687")
  .option("authentication.basic.username", "neo4j")
  .option("authentication.basic.password", "letmein!")
  .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN id(n) as id, n.name as name")
  .option("schema.strategy", "string")
  .load())

df.printSchema()
df.show()
----

.Schema output
----
root
|-- id: string (nullable = true)
|-- name: string (nullable = true)
----


.Dataframe output
|===
|id |name

|"0"|"John Doe"
|"1"|"Jane Doe"
|===

As you can see, the Struct returned by the query is made of strings.
To convert *only some* of the values, use regular Scala/Python code:

[source,scala]
----
import scala.jdk.CollectionConverters._
val result = df.collectAsList()
for (row <- result.asScala) {
  // if <some specific condition> then convert like below
  println(s"""Age is: ${row.getString(0).toLong}""")
}
----

[[read-known-problem]]
== Known problem

Because Neo4j is a schema free database, the following scenario may occur:

[source,cypher]
----
CREATE (p1:Person {age: "32"}), (p2:Person {age: 23})
----

The same field on the same node label has two different types.

Spark doesn't like it since the DataFrame requires a schema,
meaning each column of the DataFrame needs to have its own type.

[source]
----
java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Long
----

In this case you can either clean up and normalize your data, or rely on the connector to
implicitly cast values to `String`.

[NOTE]
This solution is not error-proof, you might still get errors if the values cannot be coerced to String.

When the casting operation happens, this warning appears in your log, letting you know what has happened:

[source]
----
The field "age" has different types: [String, Long]
Every value will be casted to string.
----

The safest solution is to clean your data, but that is not always possible.
This is why `schema.strategy` is introduced, and you can set to `string` to get all the values
converted to string.