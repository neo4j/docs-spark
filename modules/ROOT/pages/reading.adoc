
= Reading from Neo4j

:description: The chapter explains how to read data from a Neo4j database.

Neo4j Connector for Apache Spark allows you to read data from a Neo4j instance in three different ways:

* By node labels 
* By relationship type
* By Cypher query

== Getting started

Reading all the nodes of label `Person` from your local Neo4j instance is as simple as this:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("labels", "Person")
  .load()
  .show()
----

.Result of the above code
|===
|<id> |<labels> |name |age

|0|[Person]|John|32
|===

== Neo4j read options

.List of available read options
|===
|Setting name |Description |Default value |Required

|`query`
|Cypher query to read the data
|_(none)_
|Yes^*^

|`labels`
|List of node labels separated by colon.
The first label is to be the primary label.
|_(none)_
|Yes^*^

|`relationship`
|Type of a relationship
|_(none)_
|Yes^*^

|`schema.flatten.limit`
|Number of records to be used to create the Schema (only if APOC is not installed,
or for custom Cypher queries provided via `query` options).
|`10`
|No

|`schema.strategy`
|Strategy used by the connector in order to compute the Schema definition for the Dataset.
Possible values are `string`, `sample`.
When `string` is set, it coerces all the properties to String, otherwise it tries to sample the Neo4j's dataset.
|`sample`
|No

|`pushdown.filters.enabled`
|Enable or disable the PushdownFilters support.
|`true`
|No

|`pushdown.columns.enabled`
|Enable or disable the PushdownColumn support.
|`true`
|No

|`pushdown.aggregate.enabled`
|Enable or disable the PushdownAggregate support.
|`true`
|No

|`pushdown.limit.enabled` label:new[v.5.1]
|Enable or disable the PushdownLimit support.
|`true`
|No

|`pushdown.topN.enabled` label:new[v.5.2]
|Enable or disable the PushDownTopN support.
|`true`
|No

|`partitions`
|This defines the parallelization level while pulling data from Neo4j.

*Note*: as more parallelization does not mean better query performance, tune wisely in according to
your Neo4j installation.
|`1`
|No

4+|*Query specific options*

|`query.count`
a|Query count is used only in combination with `query` option. This is a query that returns a `count`
field like the following:
----
MATCH (p:Person)-[r:BOUGHT]->(pr:Product)
WHERE pr.name = 'An Awesome Product'
RETURN count(p) AS count
----

or *a simple number* that represents the number of records returned by `query`.
Consider that the number passed by this value represents the volume of the data pulled off Neo4j,
so use it carefully.
|_(empty)_
|No

4+|*Relationship specific options*

|`relationship.nodes.map`
|If it's set to `true`, `source` and `target` nodes are returned as Map<String, String>, otherwise we flatten the properties by returning
every single node property as column prefixed by `source` or `target`
|`false`
|No

|`relationship.source.labels`
|List of source node labels separated by colon.
|_(empty)_
|Yes

|`relationship.target.labels`
|List of target node labels separated by colon.
|_(empty)_
|Yes

|===

^*^ Just one of the options can be specified at the time.

== Read data

Reading data from a Neo4j Database can be done in three ways:

* <<read-node,Node>>
* <<read-rel,Relationship>>
* xref:reading-cypher.adoc[Custom Cypher query]

[[read-node]]
=== Node

You can read nodes by specifiying a single label, or multiple labels. Like so:

.Single label
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("labels", "Person")
  .load()
----

.Multiple labels
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("labels", "Person:Customer:Confirmed")
  .load()
----

[NOTE]
Label list can be specified both with starting colon or without it: +
`Person:Customer` and `:Person:Customer` are considered the same thing.

==== Columns

When reading data with this method, the DataFrame contains all the fields contained in the nodes,
plus two additional columns.

* `<id>` the internal Neo4j ID
* `<labels>` a list of labels for that node

==== Schema

If APOC is available, the schema is created with
link:https://neo4j.com/labs/apoc/4.1/overview/apoc.meta/apoc.meta.nodeTypeProperties/[apoc.meta.nodeTypeProperties, window=_blank].
Otherwise, we execute the following Cypher query:

[source,cypher]
----
MATCH (n:<labels>)
RETURN n
ORDER BY rand()
LIMIT <limit>
----

Where `<labels>` is the list of labels provided by `labels` option and `<limit>` is the
value provided by `schema.flatten.limit` option.
The results of such query are flattened, and the schema is created from those properties.

===== Example

[source,cypher]
----
CREATE (p1:Person {age: 31, name: 'Jane Doe'}),
    (p2:Person {name: 'John Doe', age: 33, location: null}),
    (p3:Person {age: 25, location: point({latitude: -37.659560, longitude: -68.178060})})
----

The following schema is created:

|===
|Field |Type

|<id>|Int

|<labels>|String[]

|age|Int

|name|String

|location|Point

|===

[[read-rel]]
=== Relationship

To read a relationship you must specify the relationship type, the source node labels, and the target node labels.

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()
----

This creates the following Cypher query:

[source,cypher]
----
MATCH (source:Person)-[rel:BOUGHT]->(target:Product)
RETURN source, rel, target
----

==== Node mapping
The result format can be controlled by the `relationship.nodes.map` option (default is `false`).

When it is set to `false`, source and target nodes properties are returned in separate columns
prefixed with `source.` or `target.` (i.e., `source.name`, `target.price`).

When it is set to `true`, the source and target nodes properties are returned as Map[String, String] in two columns named `source` and `target`.

[[rel-schema-no-map]]
.Nodes map set to `false`
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()
  .show()
----

.Result of the above code
|===
|<rel.id>|<rel.type>|<source.id>|<source.labels>|source.id|source.fullName|<target.id>|<target.labels>|target.name|target.id|rel.quantity

|4|BOUGHT|1|[Person]|1|John Doe|0|[Product]|Product 1|52|240
|5|BOUGHT|3|[Person]|2|Jane Doe|2|[Product]|Product 2|53|145
|===

.Nodes map set to `true`
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "true")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()
  .show()
----

.Result of the above code
|===
|<rel.id>|<rel.type>|rel.quantity|<source>|<target>

|4
|BOUGHT
|240
a|[.small]
----
{
  "fullName": "John Doe",
  "id": 1,
  "<labels>: "[Person]",
  "<id>": 1
}
----
a|[.small]
----
{
  "name": "Product 1",
  "id": 52,
  "<labels>: "[Product]",
  "<id>": 0
}
----

|4
|BOUGHT
|145
a|[.small]
----
{
  "fullName": "Jane Doe",
  "id": 1,
  "<labels>:
  "[Person]",
  "<id>": 3
}
----
a|[.small]
----
{
  "name": "Product 2",
  "id": 53,
  "<labels>: "[Product]",
  "<id>": 2
}
----
|===

[[rel-schema-columns]]
==== Columns
When reading data with this method, the DataFrame contains the following columns:

* `<id>` the internal Neo4j ID.
* `<relationshipType>` the relationship type.
* `rel.[property name]` relationship properties.

Depending on the value of `relationship.nodes.map` option.

If `true`:

* `source` the Map<String, String> of source node
* `target` the Map<String, String> of target node

If `false`:

* `<sourceId>` the internal Neo4j ID of source node
* `<sourceLabels>` a list of labels for source node
* `<targetId>` the internal Neo4j ID of target node
* `<targetLabels>` a list of labels for target node
* `source.[property name]` source node properties
* `target.[property name]` target node properties

==== Filtering

You can use Spark to filter properties of the relationship, the source node, or the target node.
Use the correct prefix:

If `relationship.nodes.map` is set to `false`:

* ``\`source.[property]` `` for the source node properties.
* ``\`rel.[property]` `` for the relationship property.
* ``\`target.[property]` `` for the target node property.

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

df.where("`source.id` = 14 AND `target.id` = 16")
----

If `relationship.nodes.map` is set to `true`:

* ``\`<source>`.\`[property]` `` for the source node map properties.
* ``\`<rel>`.\`[property]` `` for the relationship map property.
* ``\`<target>`.\`[property]` `` for the target node map property.

In this case, all the map values are to be strings, so the filter value must be a string too.

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "true")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

df.where("`<source>`.`id` = '14' AND `<target>`.`id` = '16'")
----

==== Schema

In case you're extracting a relationship from Neo4j,
the first step is to invoke the link:https://neo4j.com/labs/apoc/4.1/overview/apoc.meta/apoc.meta.relTypeProperties/[apoc.meta.relTypeProperties, window=_blank] procedure.
If APOC is not installed, we execute the following Cypher query:

[source,cypher]
----
MATCH (source:<source_labels>)-[rel:<relationship>]->(target:<target_labels>)
RETURN rel
ORDER BY rand()
LIMIT <limit>
----

Where:

* `<source_labels>` is the list of labels provided by `relationship.source.labels` option
* `<target_labels>` is the list of labels provided by `relationship.target.labels` option
* `<relationship>` is the list of labels provided by `relationship`  option
* `<limit>` is the value provided via `schema.flatten.limit`

=== Performance considerations

If the schema is not specified, the Spark Connector uses sampling as explained xref:quickstart.adoc#_schema[here] and xref:architecture.adoc#_schema_considerations[here].
Since sampling is potentially an expensive operation, consider xref:quickstart.adoc#user-defined-schema[supplying your own schema].