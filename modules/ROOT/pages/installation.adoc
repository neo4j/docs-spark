= Installation
:description: This chapter describes the quick way to get started with Neo4j Connector for Apache Spark.

[#version]
== Choose the correct version

Make sure to match both the Spark version and the Scala version of your setup.
Here is a compatibility table to help you choose the correct version of the connector.

.Compatibility table
[cols="1,1,3,2,2"]
|===
| | Group ID | Artifact ID | Version (Spark 3.0 and 3.1) | Version (Spark 3.2+)

|*Scala 2.12*
|`org.neo4j`
|`neo4j-connector-apache-spark_2.12`
|`4.1.5_for_spark_3`
|`{exact-connector-version}_for_spark_3`

|*Scala 2.13*
|`org.neo4j`
|`neo4j-connector-apache-spark_2.13`
|`4.1.5_for_spark_3`
|`{exact-connector-version}_for_spark_3`
|===

[#shell]
== Usage with the Spark shell

The connector is available via link:https://spark-packages.org/?q=neo4j-connector-apache-spark[Spark Packages]:

[.tabbed-example]
====
[.include-with-Scala]
=====
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/spark-shell --packages org.neo4j:neo4j-connector-apache-spark_{scala-version}:{exact-connector-version}_for_spark_3
----
=====

[.include-with-Python]
=====
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/pyspark --packages org.neo4j:neo4j-connector-apache-spark_{scala-version}:{exact-connector-version}_for_spark_3
----
=====
====

Alternatively, you can download the connector JAR file from the link:https://neo4j.com/product/connectors/apache-spark-connector/[Neo4j Connector Page] or from the link:https://github.com/neo4j-contrib/neo4j-spark-connector/releases[GitHub releases page] and run the following command to launch a Spark interactive shell with the connector included:

[.tabbed-example]
====
[.include-with-Scala]
=====
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/spark-shell --jars neo4j-connector-apache-spark_{scala-version}-{exact-connector-version}_for_spark_3.jar
----
=====

[.include-with-Python]
=====
[source, shell, subs="attributes+"]
----
$SPARK_HOME/bin/pyspark --jars neo4j-connector-apache-spark_{scala-version}-{exact-connector-version}_for_spark_3.jar
----
=====
====

[#applications]
== Self-contained applications

For non-Python applications:

. Include the connector in your application using the application's build tool.
. Package the application.
. Use `spark-submit` to run the application.

For Python applications, run `spark-submit` directly.

As for the `spark-shell`, you can run `spark-submit` via Spark Packages or with a local JAR file.
See the xref:quickstart.adoc#applications[Quickstart] for code examples.

[.tabbed-example]
====
[.include-with-Scala]
=====
.A minimal `build.sbt`
[source, sbt]
----
include::example$scala/build.sbt[]
----

If you use the link:https://github.com/databricks/sbt-spark-package[sbt-spark-package plugin], add the following to your `build.sbt` instead:

[source, shell, subs="attributes+"]
----
scala spDependencies += "org.neo4j/neo4j-connector-apache-spark_{scala-version}:{exact-connector-version}_for_spark_3"
----
=====

[.include-with-Java]
=====
.A minimal `pom.xml`
[source, xml]
----
include::example$java/pom.xml[]
----
=====
====

=== Other build tools

==== Gradle
[source, `build.gradle`, subs="attributes+"]
----
dependencies {
    // list of dependencies
    compile "org.neo4j:neo4j-connector-apache-spark_{scala-version}:{exact-connector-version}_for_spark_3"
}
----
